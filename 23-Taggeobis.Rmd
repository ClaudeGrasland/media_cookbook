# Geographical tags {#c23_taggeo}

```{r}
library(knitr)
library(dplyr)
library(WikidataR)
library(quanteda)
```

## Manual correction of dictionnary

The dictionnary elaboated by automatic procedures has been manually corrected and the new version is upload.

```{r}
dict<-read.table("dict/worldgeo_dict_V5.csv", 
                 header=T, 
                 sep=";",
                 encoding = "UTF-8",
                 quote = '"')
```





## Detection of geographical entities



We have  elaborated a function for the extraction of geographical units based on the dictionary elaborated in previous section (dict) according to the language (lang), the decision to split some tokens (split) to move or not to lower case (tolow)  and the possibility to add a list of compounds to be realized (comps) in order to eliminate ambiguities. 


```{r}
extract_tags <- function(qd = qd,                      # the corpus of interest
                         lang = "fr",                  # the language to be used
                         dict = dict,                  # the dictionary of target 
                         code = "id" ,                  # variable used for coding
                         split  = c("'","’","-"),       # split list
                         tolow = FALSE  ,                # Tokenize text
                         comps = c("Afrique du sud")  # compounds
                         )
{ 


  
# Tokenize  
x<-as.character(qd)


if(length(split) > 0) { reg<-paste(split, collapse = '|')
                       x <- gsub(reg," ",x)}  
if(tolow) { x <- tolower(x)} 
toks<-tokens(x)

# compounds
if(length(split) > 0) { reg<-paste(split, collapse = '|')
                       comps<- gsub(reg," ",comps)}  
if(tolow)       {comps <- tolower(comps)}  
toks<-tokens_compound(toks,pattern=phrase(comps))

  
# Load dictionaries and create compounds

  ## Target dictionary
dict<-dict[dict$lang==lang & is.na(dict$label)==F,]
target<-dict[ntoken(dict$label)>1,]
labels <-dict$label
if(length(split) > 0) { reg<-paste(split, collapse = '|')
                       labels<- gsub(reg," ",labels)}  
if(tolow)       {labels <- tolower(labels)}  
toks<-tokens_compound(toks,pattern=phrase(labels))
  
 # create quanteda dictionary
keys <-gsub(" ","_",labels)
qd_dict<-as.list(keys)
names(qd_dict)<-dict[[code]]
qd_dict<-dictionary(qd_dict,tolower = FALSE)

# Identify geo tags (states or reg or org ...)
toks_tags <- tokens_lookup(toks, qd_dict, case_insensitive = F)
toks_tags <- lapply(toks_tags, unique)
toks_tags<-as.tokens(toks_tags)
list_tags<-function(x){res<-paste(x, collapse=' ')}
docvars(qd)[["tags"]]<-as.character(lapply(toks_tags,FUN=list_tags))
docvars(qd)[["nbtags"]]<-ntoken(toks_tags)



# Export results
return(qd)
 }
 

            
```

### Google news France

```{r, eval=FALSE}


#dict<-readRDS("dict/worldgeo_dict_V4.RDS")


qd <- readRDS("data/google/fr_FRA_google_int.RDS")
docvars(qd)<-docvars(qd)[c("source","date")]
qd<-corpus_subset(qd,duplicated(as.character(qd))==FALSE)

frcomps<-c("Europe 1", "Atlantic city", 
           "Loire-Atlantique", "Pyrénées-Atlantique", "Pyrénées-Atlantiques",
           "Alpes-de-Haute-Provence", "Hautes-Alpes", "Rhöne-Alpes","Alpes-Maritimes",
           "Chantiers de l'Atlantique", "TGV Atlantique",
           "Bourse de Paris", "Paris SG", "Ville de Paris", "Grand Paris")

qd <- extract_tags (qd = qd,
                     lang="fr",
                     dict = dict,
                     code = "code",
                     split = c("'","’","-"),
                     comps = frcomps,
                     tolow = FALSE)
saveRDS(qd,"data/google/fr_FRA_google_int_geo.RDS")


table(qd$nbtags)
x<-data.frame(text=as.character(qd),regs=qd$tags,nbregs=qd$nbtags)



```
