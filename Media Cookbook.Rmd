--- 
title: "The media cookbook"
subtitle: "Working paper of the ANR-DFG Imageun Project"
author: "Claude Grasland"
date: "`r Sys.Date()`"
documentclass: book
link-citations: yes
description: "A reminder of some experiments made in 2021"
site: bookdown::bookdown_site
always_allow_html: yes
---
--- 
title: "The media cookbook"
subtitle: "Working paper of the ANR-DFG Imageun Project"
author: "Claude Grasland"
date: "`r Sys.Date()`"
documentclass: book
link-citations: yes
description: "A reminder of some experiments made in 2021"
site: bookdown::bookdown_site
always_allow_html: yes
---

```{r setup, include = FALSE, cache = FALSE}
library(knitr)
library(tidyverse)

knitr::opts_chunk$set(cache = TRUE,
                      echo = TRUE,
                      comment = "")

```



# Introduction

The aim of this cookbook is to exchange ideas and programs between members of the "Media Group" of the ANR-DFG project IMAGEUN. 


<!--chapter:end:index.Rmd-->


# Import from Media Cloud {#c11_mediacloud}

Placeholder


## Selection of media with source manager
## Checking the stability through time
## Selection of news specifically related to a topic (option)
## Download and storage of news

<!--chapter:end:11-corpus-mediacloud.Rmd-->


# quanteda corpus {#c12_mediacloud_to_quanteda}

Placeholder


## Example 
### Importation of text to R
### encoding problems
### Transformation in quanteda format
### Storage of the quanteda object
### Backtransformation of quanteda to data.table or tibble
## Germany
### Frankfurter Allgemeine Zeitung 
### Süddeutsche Zeitung
## France
### Le Figaro
### Le Monde
## Royaume-Uni
### The Guardian
### The Daily Telegraph
## Ireland
### The Irish Time
### The Belfast Telegraph
## Turkey
### The Yeni Sati
### Cumhuryet
## Tunisia
### Réalités
### L'économiste Maghrebin
### La Presse
### Babnet
## Algeria
### Al Nahar
### El Kahber
### al Chorouk
### El Watan

<!--chapter:end:12-mediacloud_to_quanteda.Rmd-->


# Wikidata {#c21_wikidata}

Placeholder


## Introduction
## Wikidata 
### Codification of entities
### Informations on entities
### Wikipedia entities as nodes of an ontolongy
### A tool for cross-linguistical experiments
## The package WikidataR
### Basic operations
#### identification of entities of interest
### Elaboration of a cross_linguistic dictionnary
### Extraction of aliases
### Conclusion

<!--chapter:end:21-Wikidata.Rmd-->


# Dictionary of geographical entities {#c22_Dicogeo}

Placeholder


## Dictionary of geographical entities
### Load the list of world regions, organisation states and capital cities
### Extract définitions
### Extract aliases and create dictionary

<!--chapter:end:22-Dicogeo.Rmd-->


# Geographical tags {#c23_taggeo}

Placeholder


## Manual correctio n of dictionnary
## Detection of geographical entities
### Le Figaro (FRA)
### Le Monde
### FAZ
### Süddeutsche Zeitung
### Guardian
### Daily Telegraph
### Belfast Telegraph
### Irish Times
### Cumhuryet
### Yeni Safak
### El Khabar (DZA)
### Al Chorouk (DZA)
### El Watan
### Global corpus

<!--chapter:end:23-Taggeo.Rmd-->

# Corpus {#c24_corpus}

```{r}
library(knitr)
library(dplyr)
library(quanteda)
library(data.table)
library(tidytext)
library(ggplot2)
library(readr)
library(stringr)
```

## Objectives

The aim of this section is to separate states and world region and to evaluate the relative frequencies of world regions by media an through time.



## Prepare data


```{r, warning=F, eval=FALSE}
qd<-readRDS("quanteda/corpus_worldgeo_V3.RDS")

# Extract regs
qd$regs<-qd$tags
qd$regs<-gsub("ST_...","",qd$tags)
qd$regs<-gsub("CA_...","",qd$regs)
qd$nbregs<-ntoken(tokens(as.character(qd$regs)))

# Extract states
qd$sta<-qd$tags
qd$sta<-str_replace_all(qd$sta, "RE_[[:alpha:]_]+", "")
qd$sta<-str_replace_all(qd$sta, "CO_[[:alpha:]_]+", "")
qd$sta<-str_replace_all(qd$sta, "OR_[[:alpha:]_]+", "")
qd$sta<-str_replace_all(qd$sta, "LA_[[:alpha:]_]+", "")
qd$sta<-str_replace_all(qd$sta, "SE_[[:alpha:]_]+", "")
qd$sta<-str_replace_all(qd$sta, "ST_", "")
qd$sta<-str_replace_all(qd$sta, "CA_", "")

qd$nbsta<-ntoken(tokens(as.character(qd$sta)))
td<-tidy(qd)


hypercube <-function(qd = qd,
                     when = "date",
                     when_cut = "year",
                     who = "source",
                     where1 = "tags",
                     where2 = "tags")
                     
  {   

# create data.table accroding to parameter chosen
  don<-docvars(qd)

  df<-data.table(id = docid(qd),
                 who = don[,who],
                 when = as.character(cut(don[,when],breaks=when_cut)),
                 where1 = don[,where1],
                 where2 = don[,where2])



# add code _no_ for empty fields
df$where1[df$where1==""]<-"_no_"
df$where2[df$where2==""]<-"_no_"


# unnest where1
  df<-unnest_tokens(df,where1,where1,to_lower=F)
  
# unnest where2
  df<-unnest_tokens(df,where2,where2,to_lower=F)  
  
# define number of occurence by id
  nb<-df[,.N,list(id)] %>%  mutate(wgt = 1/N) %>% select(-N)
  df<-df %>% left_join(nb) 
  
  rm(nb)
 
# Aggregate
  hc<- df[,.( tags = .N, news=sum(wgt)) ,.(who, when,where1,where2)]
  
# Convert date to time
  hc$when<-as.Date(hc$when)
  
# return hypercube
  return(hc)

}
# Create Regional hypercube
hc_reg <- hypercube(qd = qd,
                     when = "date",
                     when_cut = "months",
                     who = "source",
                     where1 = "regs",
                     where2 = "regs")

saveRDS(hc_reg,"hypercube/hc_reg.RDS")

# Create State hypercube
hc_sta <- hypercube(qd = qd,
                     when = "date",
                     when_cut = "months",
                     who = "source",
                     where1 = "sta",
                     where2 = "sta")

saveRDS(hc_sta,"hypercube/hc_sta.RDS")

# Create State x Reg hypercube
hc_sta_reg <- hypercube(qd = qd,
                     when = "date",
                     when_cut = "months",
                     who = "source",
                     where1 = "sta",
                     where2 = "regs")

saveRDS(hc_sta_reg,"hypercube/hc_sta_reg.RDS")




```

## Distribution des nouvelles par média au cours du temps

```{r, warnings=F, message=FALSE}
hc<-readRDS("hypercube/hc_reg.RDS")

hc$OK<-as.factor(hc$where1!="_no_") 
levels(hc$OK)<-c("Non","Oui")

month<-hc[,.(nb<-sum(news)),.(when,who,OK)] %>%
  dcast(formula = who+when~OK, value.var = "V1",fill = 0) %>%
  mutate(Media=who,
         Date = when,
         Total=Non+Oui,
         Frequence = Oui,
         Pourcentage = 100*Frequence/Total) %>%
  select(Media,Date, Total,Frequence, Pourcentage) %>%
  filter(is.na(Date)==F, Date < as.Date("2021-07-01"))


#kable(month,digits=c(0,0,0,2),caption = "Parts des nouvelles mentionnant une macro-région")

ggplot(month,aes(x=Date,y=Total,color=Media)) + 
       geom_line(lwd=0.5) +

ggtitle("Nombre de nouvelles par média (Jan. 2019-  Juin 2021)",
               subtitle = "source : Mediacloud)" )
```





## Fréquences des macro-régions par journal

```{r, warnings=F, message=FALSE}
hc<-readRDS("hypercube/hc_reg.RDS")

hc$OK<-as.factor(hc$where1!="_no_") 
levels(hc$OK)<-c("Non","Oui")

med<-hc[,.(nb<-sum(news)),.(who,OK)] %>%
  dcast(formula = who~OK, value.var = "V1") %>% 
  mutate(Media = who,
         Total=Non+Oui,
         Frequence = Oui,
         Pourcentage = 100*Frequence/Total) %>%
  select(Media, Total,Frequence, Pourcentage)

tot<-med[1,]
tot$Media<-"Total"
tot$Total<-sum(med$Total)
tot$Frequence<-sum(med$Frequence)
tot$Pourcentage<-100*tot$Frequence/tot$Total

tabres<-rbind(med,tot)
kable(tabres,digits=c(0,0,0,2),caption = "Parts des nouvelles mentionnant une macro-région")
```

 



## Fréquences des mentions de pays étrangers par journal

```{r, warnings=F, message=FALSE}
hc<-readRDS("hypercube/hc_sta.RDS")

hc$OK<-as.factor(hc$where1!="_no_" & substr(hc$who,4,6)!=hc$where1) 
levels(hc$OK)<-c("Non","Oui")

med<-hc[,.(nb<-sum(news)),.(who,OK)] %>%
  dcast(formula = who~OK, value.var = "V1") %>% 
  mutate(Media = who,
         Total=Non+Oui,
         Frequence = Oui,
         Pourcentage = 100*Frequence/Total) %>%
  select(Media, Total,Frequence, Pourcentage)

tot<-med[1,]
tot$Media<-"Total"
tot$Total<-sum(med$Total)
tot$Frequence<-sum(med$Frequence)
tot$Pourcentage<-100*tot$Frequence/tot$Total

tabres<-rbind(med,tot)
kable(tabres,digits=c(0,0,0,2),caption = "Parts des nouvelles mentionnant un pays étranger")
```


## Fréquences des mentions simaultanées d'états et de région par journal

```{r, warnings=F, message=FALSE}
hc<-readRDS("hypercube/hc_sta_reg.RDS")

hc$OK<-as.factor(hc$where1!="_no_" & hc$where2!="_no_" ) 
levels(hc$OK)<-c("Non","Oui")

med<-hc[,.(nb<-sum(news)),.(who,OK)] %>%
  dcast(formula = who~OK, value.var = "V1") %>% 
  mutate(Media = who,
         Total=Non+Oui,
         Frequence = Oui,
         Pourcentage = 100*Frequence/Total) %>%
  select(Media, Total,Frequence, Pourcentage)

tot<-med[1,]
tot$Media<-"Total"
tot$Total<-sum(med$Total)
tot$Frequence<-sum(med$Frequence)
tot$Pourcentage<-100*tot$Frequence/tot$Total

tabres<-rbind(med,tot)
kable(tabres,digits=c(0,0,0,2),caption = "Parts des nouvelles mentionnant à la fois un pays (étranger ou non) et une macro-région")
```




<!--chapter:end:24-Corpus.Rmd-->


# Top World regions {#c31_cybergeo}

Placeholder


## Data
## Top 50 regions in full corpus
### Unweighted
### Weighted
## Top 10 regions by media
### German newspapers - Top 10 regions
### French newspapers - Top 10 regions
### UK newspapers - Top 10 regions
### Irish newspapers - Top 10 regions
### Turkish newspapers - Top 10 regions
### Algerian newspapers
## Synthesis by correspondance analysis and Hierarchical Clustering
### Factor 1-2
### Factors 3-4
### Cluster analysis (world regions)
### Cluster analysis (medias)
## Synthesis by chi-square & heatmap

<!--chapter:end:31-TopRegions.Rmd-->


# Geonetworks {#c31_geonetworks}

Placeholder


## All corpus
### Load Hypercube State x Regions
###  Select links
### Create interaction matrix
### Estimate random model
#### Application
### Visualization
#### Function
#### Raw network
#### Preferential links 
## By country
### France
### Germany
### Turkey
### United Kingdom (except Ulster)
### Ireland & Ulster
### Algeria

<!--chapter:end:32_geonetworks.Rmd-->

