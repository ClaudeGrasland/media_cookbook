[["index.html", "The media cookbook Working paper of the ANR-DFG Imageun Project Chapter 1 Introduction", " The media cookbook Working paper of the ANR-DFG Imageun Project Claude Grasland 2021-07-14 Chapter 1 Introduction The aim of this cookbook is to exchange ideas and programs between members of the “Media Group” of the ANR-DFG project IMAGEUN. "],["21-corpus-mediacloud.html", "Chapter 2 Extraction of data from Media Cloud 2.1 Selection of media with source manager 2.2 Checking the stability through time 2.3 Selection of news specifically related to a topic (option) 2.4 Download and storage of news", " Chapter 2 Extraction of data from Media Cloud library(quanteda) Package version: 3.0.0 Unicode version: 13.0 ICU version: 69.1 Parallel computing: 8 of 8 threads used. See https://quanteda.io for tutorials and examples. library(dplyr) library(ggplot2) library(tidytext) library(knitr) knitr::opts_chunk$set(cache = TRUE, echo = TRUE, comment = &quot;&quot;) (tbd : presentation of the MediaCloud project) Mediacloud can be freely used by researchers. All you have to do is to create an account at the following adress : https://explorer.mediacloud.org You have different ways to get title of news. We will focus here on a simple example of data obtained through the mediacloud interface. We suppose that you want to extract news from the Tunisian newspapers speaking from Europe. 2.1 Selection of media with source manager We use the application called Source Manager and we introduce a research by collection which is the most convenient to explore what is available in a country. In our example, the target country is Tunisia and we have three collections that are propsed : We have selected the collection named “Tunisia National” because we are interested in the most important newspapers of the country. The buble graphic on the right indicates immediately the media that has produced the highest number of news, but it is wise to explore in more details the list on the left which indicates for each media the statting date of data collection. When a media appears interesting, we click on its name to obtain a brief summary of the metadata. For example, in the case of L’économiste Maghrebin the metadata indicates : The media looks promising, but before to go further, it can be better to have a look at the website of the media to have a more concrete idea of the content if we don’t know in advance what it is about in terms of content, what is the ideological orientation, etc. Here we can see that this is an ecnomic journal, published in french, with news organized in concentric geographic circles (Nation &gt; Maghreb &gt; Africa &gt; World) which is precisely what we are looking for in the IMAGEUN project. We will further complete the informations about this, but before to do that we have to check in more details if the production of the media is regular through time with another tool offered by mediacloud, the explorer. 2.2 Checking the stability through time We have clicked on search in explorer on the metadata page of the Source Manager and obtain a news interfacce where we modify the date to cover the full period of collection of the media (or our period of interest). In the research field, we let the search term * which indicates a research on all news. Below your request, you obtain a graphic entitled Attention Over Time with the distribution of the number of news published per day which help you to verify if the distribution of news is regular through time. You just have to modify the type of graphic in order to visualize Story Count and you can choose the time span you want (day, week or month) for the evaluation of the regularity of news flow. In our example, we notice that at daily level they are some brief period of break in 2019, but the flow is reasonnabely regular with approximatively 5 news per day at the beginning and 10 to 20 in the final period. We also notice a classical week cycle with a decrease of news published during the week-end. Going down, you will find a news panel entitled Total Attention which gives you the total number of stories found. In our example, we have a total of 13626 stories produced by our media over the period. 2.3 Selection of news specifically related to a topic (option) You can eventually use Mediacloud to check the number of news produced about a specific topic, for example Europe or European Union or EU. The request shouldbe put in lower case with \"\" for compounds. Detailed explanation are available in the query guide. This time you can use the graphic option Stories percentage rather than Story count if you want to viusalize the salience of the topic through time. In our example, we have 369 news that appears to be related to our request about Europe or EU with a relatively regular pattern at month level of 1 to 3 % of news and exceptionaly 5 to 7 %. 2.4 Download and storage of news According to your selection (all news or a specific topic) you will download more or less title. Here, me make the choice to get all news, which means that we have to repeat the original request with *. Finally, by clicking on the button Download all story URLS, you can get a .csv file that you can easily load in your favorite programming language as we will see in the next section. "],["22-mediacloud_to_quanteda.html", "Chapter 3 Creation of a quanteda corpus 3.1 Importation of text to R 3.2 Cleaning of the text 3.3 Transformation in quanteda format", " Chapter 3 Creation of a quanteda corpus library(quanteda) library(dplyr) library(ggplot2) library(tidytext) library(knitr) library(readtext) knitr::opts_chunk$set(cache = TRUE, echo = TRUE, comment = &quot;&quot;) In the previous section (ref…) whe have obtained a .csv file of news collected from MediaCloud. We will try now to put this data in a standard form and we have chosen the format of the quanteda package as reference for data organization and storage. But of course the researchers involved in the project can prefer to use other R packages like tm or tidytext. And they can also prefer to use another programming language for Python. It is the reason why we explain how to transform and export the data that has been prepared and harmonized with quanteda in various format like .csv or JSON. 3.1 Importation of text to R This step is really not obvious because many problems of encoding can appear that are more or less easy to solve. In principle , the data from Media Cloud are exported in standard UTF-8 but as we will see it is not necessary the case. 3.1.1 Import with R-Base We try firstly to use the standard R function read.csv(): dt&lt;-read.csv(file =(&quot;data/corpora/TUN/-all-story-urls-20210713153555.csv&quot;), sep=&quot;,&quot;, header=T, encoding = &quot;UTF-8&quot;, stringsAsFactors = F) head(dt) stories_id publish_date 1 719049395 2017-11-03 06:59:58 2 719049401 2017-11-03 06:15:37 3 719049385 2017-11-03 07:32:40 4 719049416 2017-11-03 04:36:14 5 719457836 2017-11-04 07:12:34 6 719457840 2017-11-04 06:38:38 title 1 Université: projet de formation codiplomante entre la Sorbonne et Tunis El Manar 2 Cyrine Ben Hassine élue présidente de l’ATUGE France (2017-2018) 3 Santé : pour une nouvelle relation médecin-patient 4 Düsseldorf : 250 visiteurs professionnels et 8 exposants tunisiens au salon Medica 2017 5 Etude: 68% des dirigeants n’ont pas creusé la transmission à la 3ème génération 6 Réconciliation url 1 http://www.leconomistemaghrebin.com/2017/11/03/universite-projet-de-formation-co-diplomante-entre-sorbonne-tunis-el-manar/ 2 http://www.leconomistemaghrebin.com/2017/11/03/cirine-ben-hassine-elue-presidente-latuge-france-2017-2018/ 3 http://www.leconomistemaghrebin.com/2017/11/03/sante-medecin-patient/ 4 http://www.leconomistemaghrebin.com/2017/11/03/dusseldorf-250-visiteurs-professionnels-7-exposants-tunisiens-salon-medica-2017/ 5 http://www.leconomistemaghrebin.com/2017/11/04/dirigeants-transmission-3eme-generation/ 6 http://www.leconomistemaghrebin.com/2017/11/04/reconciliation/ language ap_syndicated themes media_id media_name 1 fr False 623820 L&#39;Economiste Maghrebin 2 fr False 623820 L&#39;Economiste Maghrebin 3 fr False 623820 L&#39;Economiste Maghrebin 4 fr False 623820 L&#39;Economiste Maghrebin 5 fr False 623820 L&#39;Economiste Maghrebin 6 fr False 623820 L&#39;Economiste Maghrebin media_url 1 http://www.leconomistemaghrebin.com/ 2 http://www.leconomistemaghrebin.com/ 3 http://www.leconomistemaghrebin.com/ 4 http://www.leconomistemaghrebin.com/ 5 http://www.leconomistemaghrebin.com/ 6 http://www.leconomistemaghrebin.com/ str(dt) &#39;data.frame&#39;: 13623 obs. of 10 variables: $ stories_id : int 719049395 719049401 719049385 719049416 719457836 719457840 719457843 719457851 719489735 720908005 ... $ publish_date : chr &quot;2017-11-03 06:59:58&quot; &quot;2017-11-03 06:15:37&quot; &quot;2017-11-03 07:32:40&quot; &quot;2017-11-03 04:36:14&quot; ... $ title : chr &quot;Université: projet de formation codiplomante entre la Sorbonne et Tunis El Manar&quot; &quot;Cyrine Ben Hassine élue présidente de l’ATUGE France (2017-2018)&quot; &quot;Santé : pour une nouvelle relation médecin-patient&quot; &quot;Düsseldorf : 250 visiteurs professionnels et 8 exposants tunisiens au salon Medica 2017&quot; ... $ url : chr &quot;http://www.leconomistemaghrebin.com/2017/11/03/universite-projet-de-formation-co-diplomante-entre-sorbonne-tunis-el-manar/&quot; &quot;http://www.leconomistemaghrebin.com/2017/11/03/cirine-ben-hassine-elue-presidente-latuge-france-2017-2018/&quot; &quot;http://www.leconomistemaghrebin.com/2017/11/03/sante-medecin-patient/&quot; &quot;http://www.leconomistemaghrebin.com/2017/11/03/dusseldorf-250-visiteurs-professionnels-7-exposants-tunisiens-sa&quot;| __truncated__ ... $ language : chr &quot;fr&quot; &quot;fr&quot; &quot;fr&quot; &quot;fr&quot; ... $ ap_syndicated: chr &quot;False&quot; &quot;False&quot; &quot;False&quot; &quot;False&quot; ... $ themes : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... $ media_id : int 623820 623820 623820 623820 623820 623820 623820 623820 623820 623820 ... $ media_name : chr &quot;L&#39;Economiste Maghrebin&quot; &quot;L&#39;Economiste Maghrebin&quot; &quot;L&#39;Economiste Maghrebin&quot; &quot;L&#39;Economiste Maghrebin&quot; ... $ media_url : chr &quot;http://www.leconomistemaghrebin.com/&quot; &quot;http://www.leconomistemaghrebin.com/&quot; &quot;http://www.leconomistemaghrebin.com/&quot; &quot;http://www.leconomistemaghrebin.com/&quot; ... The importation was successfull for 13623 news but message of errors appeared for 3 news where R sent a message of error telling : Error in gregexpr(calltext, singleline, fixed = TRUE) : regular expression is invalid UTF-8 Looking in more details, we discover also some problems of encoding in news like in the following example where the text of the news appears differently if we apply the standard functions paste() o0 the specialized function r knitr::kable for printing. paste(dt[12, 3]) [1] &quot;Noureddine Bhiri: &amp;#171;&amp;#160;Nous soutenons le PLF relatif à la protection des forces armées&amp;#160;&amp;#187;&quot; kable((dt[12,3])) x Noureddine Bhiri: « Nous soutenons le PLF relatif à la protection des forces armées » 3.1.2 Import with readtext The package readtext which has been developped by the same team as quanteda appears more efficient because it is adapted to a great diversity of input format and introduce immediately some parameters of interest for the creation of quanteda corpora like : text_field : the column describing the text docid_field : the unique code of documents The resulting object belong to the class data.tablebut also the class readtextwhich is more easy to transform in quanteda corpus. df &lt;- readtext(file =&quot;data/corpora/TUN/-all-story-urls-20210713153555.csv&quot;, encoding = &quot;UTF-8&quot;, text_field = &quot;title&quot;, docid_field = &quot;stories_id&quot;) class(df) [1] &quot;readtext&quot; &quot;data.frame&quot; In our example, the use of this package does not solve the problem of encoding but this problem is apparently limited to a small number of news and a small number of characters. It can therefore be solved in this example by manual corrections. 3.2 Cleaning of the text in our opinion it is better to proceed to some cleaing operation before to import the text in quanteda and to create a news version of the initial .csv file. 3.2.1 encoding problems It is sometime possible to adapt manually the encoding problem whan they are not too much as in present example. # standardize apostrophe df$text&lt;-gsub(&quot;&amp;#8217;&quot;,&quot;&#39;&quot;,df$text) # standardize punct df$text&lt;-gsub(&#39;&amp;#8230;&#39;,&#39;.&#39;,df$text) # standardize hyphens df$text&lt;-gsub(&#39;&amp;#8211;&#39;,&#39;-&#39;,df$text) # Remove quotation marks df$text&lt;-gsub(&#39;&amp;#171;&amp;#160;&#39;,&#39;&#39;,df$text) df$text&lt;-gsub(&#39;&amp;#160;&amp;#187;&#39;,&#39;&#39;,df$text) df$text&lt;-gsub(&#39;&amp;#8220;&#39;,&#39;&#39;,df$text) df$text&lt;-gsub(&#39;&amp;#8221;&#39;,&#39;&#39;,df$text) df$text&lt;-gsub(&#39;&amp;#8216;&#39;,&#39;&#39;,df$text) df$text&lt;-gsub(&#39;&amp;#8243;&#39;,&#39;&#39;,df$text) 3.2.2 Other cleaning actions We can introduce othr cleaning procedures here or keep it for later analysis 3.3 Transformation in quanteda format We propose a storage based on quanteda format by just transforming the data that has been produced by readtext. We keep only the name of the source and the date of publication. # Create Quanteda corpus qd&lt;-corpus(df) # Select docvar fields and rename media qd$date &lt;-as.Date(qd$publish_date) qd$source &lt;-&quot;fr_TUN_ecomag&quot; docvars(qd)&lt;-docvars(qd)[,c(&quot;source&quot;,&quot;date&quot;)] # Add global meta meta(qd,&quot;meta_source&quot;)&lt;-&quot;Media Cloud &quot; meta(qd,&quot;meta_time&quot;)&lt;-&quot;Download the 2021-07-13&quot; meta(qd,&quot;meta_author&quot;)&lt;-&quot;Elaborated by Claude Grasland&quot; meta(qd,&quot;project&quot;)&lt;-&quot;ANR-DFG Project IMAGEUN&quot; We have created a quanteda object with a lot of information stored in various fields. The structure of the object is the following one str(qd) &#39;corpus&#39; Named chr [1:13623] &quot;Université: projet de formation codiplomante entre la Sorbonne et Tunis El Manar&quot; ... - attr(*, &quot;names&quot;)= chr [1:13623] &quot;719049395&quot; &quot;719049401&quot; &quot;719049385&quot; &quot;719049416&quot; ... - attr(*, &quot;docvars&quot;)=&#39;data.frame&#39;: 13623 obs. of 5 variables: ..$ docname_: chr [1:13623] &quot;719049395&quot; &quot;719049401&quot; &quot;719049385&quot; &quot;719049416&quot; ... ..$ docid_ : Factor w/ 13623 levels &quot;719049395&quot;,&quot;719049401&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ..$ segid_ : int [1:13623] 1 1 1 1 1 1 1 1 1 1 ... ..$ source : chr [1:13623] &quot;fr_TUN_ecomag&quot; &quot;fr_TUN_ecomag&quot; &quot;fr_TUN_ecomag&quot; &quot;fr_TUN_ecomag&quot; ... ..$ date : Date[1:13623], format: &quot;2017-11-03&quot; &quot;2017-11-03&quot; ... - attr(*, &quot;meta&quot;)=List of 3 ..$ system:List of 6 .. ..$ package-version:Classes &#39;package_version&#39;, &#39;numeric_version&#39; hidden list of 1 .. .. ..$ : int [1:3] 3 0 0 .. ..$ r-version :Classes &#39;R_system_version&#39;, &#39;package_version&#39;, &#39;numeric_version&#39; hidden list of 1 .. .. ..$ : int [1:3] 4 0 2 .. ..$ system : Named chr [1:3] &quot;Darwin&quot; &quot;x86_64&quot; &quot;claudegrasland1&quot; .. .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;sysname&quot; &quot;machine&quot; &quot;user&quot; .. ..$ directory : chr &quot;/Users/claudegrasland1/git/media_cookbook&quot; .. ..$ created : Date[1:1], format: &quot;2021-07-14&quot; .. ..$ source : chr &quot;data.frame&quot; ..$ object:List of 2 .. ..$ unit : chr &quot;documents&quot; .. ..$ summary:List of 2 .. .. ..$ hash: chr(0) .. .. ..$ data: NULL ..$ user :List of 4 .. ..$ meta_source: chr &quot;Media Cloud &quot; .. ..$ meta_time : chr &quot;Download the 2021-07-13&quot; .. ..$ meta_author: chr &quot;Elaborated by Claude Grasland&quot; .. ..$ project : chr &quot;ANR-DFG Project IMAGEUN&quot; We can look at the first titles with head() kable(head(qd,3)) x 719049395 Université: projet de formation codiplomante entre la Sorbonne et Tunis El Manar 719049401 Cyrine Ben Hassine élue présidente de l’ATUGE France (2017-2018) 719049385 Santé : pour une nouvelle relation médecin-patient We can get meta information on each stories with summary() summary(head(qd,3)) Corpus consisting of 3 documents, showing 3 documents: Text Types Tokens Sentences source date 719049395 13 13 1 fr_TUN_ecomag 2017-11-03 719049401 11 11 1 fr_TUN_ecomag 2017-11-03 719049385 7 7 1 fr_TUN_ecomag 2017-11-03 We can get meta information about the full document meta(qd) $meta_source [1] &quot;Media Cloud &quot; $meta_time [1] &quot;Download the 2021-07-13&quot; $meta_author [1] &quot;Elaborated by Claude Grasland&quot; $project [1] &quot;ANR-DFG Project IMAGEUN&quot; 3.3.1 Storage of the quanteda object We can finally save the object in .Rdata format in a directory dedicated to our quanteda files. It can be usefull to give some information in the name of the file saveRDS(qd,&quot;data/corpora/TUN/fr_TUN_ecomag.Rdata&quot;) We have kept all the information present in the initial file, but also added specific metadata of interest for us. The size of the storage is now equal to 0.6 Mb which means a division by 6 as compared to the initial .csv file downloaded from Media Cloud. 3.3.2 Backtransformation of quanteda to data.table or tibble In the following steps, we will make an intensive use of quanteda, but sometimes it can be usefull to export the results in a more practical format or to use other packages. For this reasons, it is important to know that the tidytextpackage can easily transform quanteda object in tibbles which are more classical and easy to manage td &lt;- tidy(qd) Warning: &#39;texts.corpus&#39; is deprecated. Use &#39;as.character&#39; instead. See help(&quot;Deprecated&quot;) head(td) # A tibble: 6 x 3 text source date &lt;chr&gt; &lt;chr&gt; &lt;date&gt; 1 Université: projet de formation codiplomante entre la S… fr_TUN_ec… 2017-11-03 2 Cyrine Ben Hassine élue présidente de l’ATUGE France (2… fr_TUN_ec… 2017-11-03 3 Santé : pour une nouvelle relation médecin-patient fr_TUN_ec… 2017-11-03 4 Düsseldorf : 250 visiteurs professionnels et 8 exposant… fr_TUN_ec… 2017-11-03 5 Etude: 68% des dirigeants n’ont pas creusé la transmiss… fr_TUN_ec… 2017-11-04 6 Réconciliation fr_TUN_ec… 2017-11-04 str(td) tibble [13,623 × 3] (S3: tbl_df/tbl/data.frame) $ text : chr [1:13623] &quot;Université: projet de formation codiplomante entre la Sorbonne et Tunis El Manar&quot; &quot;Cyrine Ben Hassine élue présidente de l’ATUGE France (2017-2018)&quot; &quot;Santé : pour une nouvelle relation médecin-patient&quot; &quot;Düsseldorf : 250 visiteurs professionnels et 8 exposants tunisiens au salon Medica 2017&quot; ... $ source: chr [1:13623] &quot;fr_TUN_ecomag&quot; &quot;fr_TUN_ecomag&quot; &quot;fr_TUN_ecomag&quot; &quot;fr_TUN_ecomag&quot; ... $ date : Date[1:13623], format: &quot;2017-11-03&quot; &quot;2017-11-03&quot; ... "],["31_geonetworks.html", "Chapter 4 Geonetworks 4.1 Geotags 4.2 Hypercubes 4.3 Linkages 4.4 Visualization", " Chapter 4 Geonetworks We present here the program used for the elaboration of networks of states and regions that has been presented at the 3rd IMAGEUN workshop in Paris the 12-13 July 2021. 4.1 Geotags During this first step, we proceed in three steps to (1) the selection of the corpus, (2) the tokenization and cleaning of the text and (3) the creation of compounds and the extraction of tags related togeographical names. 4.1.1 Select corpus We load a coprus of title of news stored in quanteda forma # Load external corpus qd&lt;-readRDS(&quot;data/corpora/TUN/fr_TUN_ecomag.Rdata&quot;) 4.1.2 Tokenize We proceed to a first tokenization with minimal option and we save the results in a file as the time of computation is very long. In the french case, we eliminate the apostrophe # Tokenize text toks&lt;-tokens(qd) # Split french apostroph toks&lt;- tokens_split(toks, &quot;&#39;&quot;) 4.1.3 identify geotags We produce a list of tags based on a dictionary. To do that it is necessary to transform the keywords in blankspace into compounds. It is also important to add external compounds that can be a problem. List of tags are produced for the whole dictionary but also for the different subtypes. Tags are uique at sentence level. # Load tags dictionary tags&lt;-read.csv2(&quot;data/corpora/TUN/geo_names_fr_V2.csv&quot;, header=T, sep=&quot;;&quot;) # create compounds presents in the dictionnary tags$ntoks&lt;-ntoken(tokens(tags$tokens)) sel&lt;-tags[tags$ntoks&gt;1,] toks2&lt;-tokens_compound(toks, pattern=phrase(sel$tokens)) # Create other compounds of interest not present in dictionary (e.g. Europe 1) comp &lt;- phrase(c(&quot;europe 1&quot;, &quot;france inter&quot;, &quot;union européenne&quot;, &quot;conseil de l europe&quot;, &quot;agence france presse&quot;)) toks2&lt;-tokens_compound(toks2, pattern=comp) # Build dictionnary keys &lt;-gsub(&quot; &quot;,&quot;_&quot;,tags$tokens) dict&lt;-as.list(keys) names(dict)&lt;-tags$code dico&lt;-dictionary(dict) # Identify geo tags (states or reg or org ...) toks_tags &lt;- tokens_lookup(toks2, dico) toks_tags &lt;- lapply(toks_tags, unique) toks_tags&lt;-as.tokens(toks_tags) list_tags&lt;-function(x){res&lt;-paste(x, collapse=&#39; &#39;)} qd$tags&lt;-as.character(lapply(toks_tags,FUN=list_tags)) qd$nbtags&lt;-ntoken(toks_tags) # Save corpus saveRDS(qd,&quot;data/corpora/TUN/fr_TUN_ecomag_geo.Rdata&quot;) 4.2 Hypercubes During this second step, we will produce aggregates of news called hypercubes which provide a number of to links between geographical units found in the text. Different options are possible which desserve the creation of a function with parameters. 4.2.1 Load data We start from the results of the previous step and we load the dictionary used for tagging and the tags describing the texts # Load quanteda corpus with geotags qd&lt;-readRDS(&quot;data/corpora/TUN/fr_TUN_ecomag_geo.Rdata&quot;) 4.2.2 Hypercube Function We start by the creation of a global hypercube crossing all the geographical tags by media and data, whatever their type (state or region). hypercube &lt;-function(qd = qd, when = &quot;date&quot;, when_cut = &quot;year&quot;, who = &quot;source&quot;, where1 = &quot;tags&quot;, where2 = &quot;tags&quot;) { # create data.table accroding to parameter chosen don&lt;-docvars(qd) df&lt;-data.table(id = docid(qd), who = don[,who], when = as.character(cut(don[,when],breaks=when_cut)), where1 = don[,where1], where2 = don[,where2]) # add code _no_ for empty fields df$where1[df$where1==&quot;&quot;]&lt;-&quot;_no_&quot; df$where2[df$where2==&quot;&quot;]&lt;-&quot;_no_&quot; # unnest where1 df&lt;-unnest_tokens(df,where1,where1,to_lower=F) # unnest where2 df&lt;-unnest_tokens(df,where2,where2,to_lower=F) # define number of occurence by id nb&lt;-df[,.N,list(id)] %&gt;% mutate(wgt = 1/N) %&gt;% select(-N) df&lt;-df %&gt;% left_join(nb) rm(nb) # Aggregate hc&lt;- df[,.( tags = .N, news=sum(wgt)) ,.(who, when,where1,where2)] # Convert date to time hc$when&lt;-as.Date(hc$when) # return hypercube return(hc) } 4.2.3 Build hypercube Depending on the hypothesis, different types of hypercubes can be quickly created with the function. This hypercube is dedicated to the creation of a full network linking geographical tags. We present here an example where we mix all geo names (states and regions together). But we can further create differences between links relating states, links relating regions, linkes relating states and regions, etc. hc_geo_geo &lt;- hypercube(qd = qd, when = &quot;date&quot;, when_cut = &quot;months&quot;, who = &quot;source&quot;, where1 = &quot;tags&quot;, where2 = &quot;tags&quot;) head(hc_geo_geo) FALSE who when where1 where2 tags news FALSE 1: fr_TUN_ecomag 2017-11-01 _no_ _no_ 126 126.00 FALSE 2: fr_TUN_ecomag 2017-11-01 FRA FRA 1 1.00 FALSE 3: fr_TUN_ecomag 2017-11-01 QAT QAT 1 1.00 FALSE 4: fr_TUN_ecomag 2017-11-01 TUN TUN 11 10.25 FALSE 5: fr_TUN_ecomag 2017-11-01 AF_subsa AF_subsa 1 1.00 FALSE 6: fr_TUN_ecomag 2017-11-01 LBN LBN 1 1.00 4.3 Linkages The objective of this step is to propose a random model of allocation of linkages between two groups of geographical units that can be the same or different. We start from an hypercube (see. step 2) which is transformed into an interaction matrix where a double constraint model is applied. Different options are introduced, in particular for the filtering of geographical units wit small number of occurences. As in previous section, we propose a function dedicated to the task of estimation of the model of interest. 4.3.1 Select links We want to produce a selection of the hypercube data set and to normalize the datain order to keep only two columns of location corresponding to where1, where2. The function will also help us to select data according to the dimensions who and when and the possible selection or exclusion of elements from where1 and where2 hc_filter &lt;- function(don = hc, who = &quot;who&quot;, when = &quot;when&quot;, where1 = &quot;where1&quot;, where2 = &quot;where2&quot;, wgt = &quot;tags&quot;, self = FALSE, when_start = NA, when_end = NA, who_exc = NA, who_inc = NA, where1_exc = NA, where1_inc = NA, where2_exc = NA, where2_inc = NA) { df&lt;-data.table(who = don[[who]], when = don[[when]], where1 = don[[where1]], where2 = don[[where2]], wgt = don[[wgt]]) # Select time period if (is.na(when_start)==FALSE) { df &lt;- df[when &gt;= as.Date(when_start), ]} if (is.na(when_end)==FALSE) { df &lt;- df[when &lt;= as.Date(when_end), ]} # Select who if (is.na(who_exc)==FALSE) { df &lt;- df[!(who %in% who_exc), ]} if (is.na(who_inc)==FALSE) { df &lt;- df[(who %in% who_inc), ]} # Select where1 if (is.na(where1_exc)==FALSE) { df &lt;- df[!(where1 %in% where1_exc), ]} if (is.na(where1_inc)==FALSE) { df &lt;- df[(where1 %in% where1_inc), ]} # Select where2 if (is.na(where2_exc)==FALSE) { df &lt;- df[!(where2 %in% where2_exc), ]} if (is.na(where2_inc)==FALSE) { df &lt;- df[(where2 %in% where2_inc), ]} # eliminate internal links if (self==FALSE) { df &lt;- df[(where1 != where2), ]} return(df) } Application now: hc&lt;-hc_filter(don = hc_geo_geo, wgt = &quot;tags&quot;, where1_exc = c(&quot;_no_&quot;), where2_exc = c(&quot;_no_&quot;), self = FALSE ) ` 4.3.2 Create interaction matrix Now, we transform the data in a matrix of interaction (i,j,Fij) whre we fill with 0 the empty cells. After that we eliminate the lines and columns wich do not fulfill one of the two conditions of salience and diversity : Salience (s1,s2) : For example, we will eliminate a country that has not be mentioned at less 20 times in the news. To do that, we compute the marginal sums of the matrix in lines (1) and columns (2) and we check if the condition is fulfilled. Diversity (n1,n2,k) : We consider that a linkage between units i and j exist if Fij &gt; k. Then we compute the number of links where the condition is verified in lines (1) and columns. To do that we compute a boolean matrix where the value are 0 if Fij &lt; k and 1 if Fi &gt;= k. And we verify if the marginal sums of the matrix verify the conditions n1 and n2. The default values are s1 = s2 = 5 n1 = n2 = 3 k = 0 They depend from the type of weight used. The previous parameter are adapted to the case where we use fractions of news as weighting criteria. They should be higher if we decide to use the number of tags. To do that, we create a specific function called build_mat . build_int &lt;- function(don = don, # a dataframe with columns i, j , Fij i = &quot;where1&quot;, j = &quot;where2&quot;, Fij = &quot;wgt&quot;, s1 = 1, s2 = 1, n1 = 1, n2 = 1, k = 0) { df&lt;-data.table(i=don[[i]],j=don[[j]],Fij=don[[Fij]]) int &lt;-df[,.(Fij=sum(Fij)),.(i,j)] int&lt;-dcast(int,formula = i~j,fill = 0) mat&lt;-as.matrix(int[,-1]) row.names(mat)&lt;-int$i mat&lt;-mat[apply(mat,1,sum)&gt;=s1,apply(mat,2,sum)&gt;=s2 ] m0&lt;-mat m0[m0&lt;k]&lt;-0 m0[m0&gt;=k]&lt;-1 mat&lt;-mat[apply(m0,1,sum)&gt;=n1,apply(m0,2,sum)&gt;=n2 ] int&lt;-melt(mat) names(int) &lt;-c(&quot;i&quot;,&quot;j&quot;,&quot;Fij&quot;) return(int) } Application now: int &lt;- build_int(don = hc, s1=1, s2=1, n1=1, n2=1, k=0) head(int) FALSE i j Fij FALSE 1 AFRIQUE AFRIQUE 0 FALSE 2 AF_nord AFRIQUE 1 FALSE 3 AGO AFRIQUE 0 FALSE 4 AMERIQUE AFRIQUE 0 FALSE 5 ARG AFRIQUE 0 FALSE 6 ASIE AFRIQUE 0 4.3.3 Estimate random model We can now solve the model and compute the most important and significant residuals according to various criteria. We will therefore build a model where i, j and Fij will be completed by a column Eij which describe the expected number of news where i and j would be associated if the allocation was made randomly. It is important to check if the size of the matrix is not too large, in order to avoid computational crash … As a security, we introduce a parameter maxsize = 100000 defined by the product of numer of lines and columns… If you decide to increase this parameter, it is at your own risk … When i and j describe the same spatial units (e.g. state x state or reg x reg), it can be important to eliminate the diagonal if it is empty or if we decide to focus only on external relations between geographical objects. For this reason, we introduce a parameter diag = TRUE or FALSE in the function rand_int() that solves the model We can eventually add to the data set further columns related to the residuals measures in different forms if we activate the parameter resid=TRUE : Rabs_ij = Fij-Eij : absolute residuals Rrel_ij = Fij/Eij : relative residuals Rchi_ij = sign(R_absij) x (Rabs_ij)^2 / Eij rand_int &lt;- function(int = int, # A table with columns i, j Fij maxsize = 100000, diag = FALSE, resid = FALSE) { # Eliminate diagonal ? if (diag==FALSE) { int &lt;- int[as.character(int$i) != as.character(int$j), ]} # Compute model if size not too large if (dim(int)[1] &lt; maxsize) { # Proceed to poisson regression model mod &lt;- glm( formula = Fij ~ i + j,family = &quot;poisson&quot;, data = int) # Add residuals if requested if(resid == TRUE) { # Add estimates int$Eij &lt;- mod$fitted.values # Add absolute residuals int$Rabs_ij &lt;- int$Fij-int$Eij # Add relative residuals int$Rrel_ij &lt;- int$Fij/int$Eij # Add chi-square residuals int$Rchi_ij &lt;- (int$Rabs_ij)**2 / int$Eij int$Rchi_ij[int$Rabs_ij&lt;0]&lt;- -int$Rchi_ij[int$Rabs_ij&lt;0] } } else { paste (&quot;Table &gt; 100000 - \\n modify maxsize = parameter \\n if you are sure that your computer can do it !&quot;)} # Export results int$i&lt;-as.character(int$i) int$j&lt;-as.character(int$j) return(int) } mod&lt;-rand_int(int, resid = TRUE, diag = FALSE) We can have a look at the more important positive residuals in absolute value : tab&lt;-mod[order(-mod$Rabs_ij),] tab&lt;-tab[(tab$i &gt; tab$j),] kable(head(tab,10) , digits=2, caption = &quot;Positive residuals&quot;) Table 4.1: Positive residuals i j Fij Eij Rabs_ij Rrel_ij Rchi_ij 68 XX_moyori AFRIQUE 6 0.35 5.65 17.26 91.89 1410 TUN DZA 16 11.59 4.41 1.38 1.68 1339 TUN DEU 13 9.23 3.77 1.41 1.54 2688 TUN JPN 8 4.58 3.42 1.75 2.56 2759 TUN KOR 7 4.00 3.00 1.75 2.24 1200 USA CHN 3 0.27 2.73 11.00 27.26 2830 TUN LBY 16 13.37 2.63 1.20 0.52 2617 TUN JOR 6 3.43 2.57 1.75 1.93 2901 TUN MAR 11 8.65 2.35 1.27 0.64 1765 TUN GBR 11 8.65 2.35 1.27 0.64 4.4 Visualization 4.4.1 Function We have created a prototype of geonetwork visualization function with different parameters making possible to display different graphs. geo_network&lt;- function(don = don, from = &quot;i&quot;, to = &quot;j&quot;, size = &quot;Fij&quot;, minsize = 1, maxsize = NA, test = &quot;Fij&quot;, mintest = 1, loops = FALSE, title = &quot;Network&quot;) { int&lt;-data.frame(i = as.character(don[,from]), j = as.character(don[,to]), size = don[,size], test = don[,test] ) if (is.na(minsize)==FALSE) {int =int[int$size &gt;= minsize,]} if (is.na(maxsize)==FALSE) {int =int[int$size &lt;= maxsize,]} if (is.na(mintest)==FALSE) {int =int[int$test &gt;= mintest,]} nodes&lt;-data.frame(code = unique(c(int$i,int$j))) nodes$code&lt;-as.character(nodes$code) nodes$id&lt;-1:length(nodes$code) nodes$label&lt;-nodes$code # Adjust edge codes edges &lt;- int %&gt;% mutate(width = 5+30*size / max(size)) %&gt;% left_join(nodes %&gt;% select(i=code, from = id)) %&gt;% left_join(nodes %&gt;% select(j=code, to = id )) # compute nodesize tot&lt;-int %&gt;% group_by(i) %&gt;% summarize(size =sum (size)) %&gt;% select (code=i,size) tot$code&lt;-as.factor(tot$code) nodes &lt;- left_join(nodes,tot) %&gt;% mutate(value = sqrt(10 *size/max(size))) #sel_nodes &lt;-nodes %&gt;% filter(code %in% unique(c(sel_edges$i,sel_edges$j))) # eliminate loops if(loops == FALSE) {edges &lt;- edges[edges$from &lt; edges$to,]} net&lt;- visNetwork(nodes, edges, main = title, height = &quot;700px&quot;, width = &quot;100%&quot;) %&gt;% visNodes(scaling =list(min =20, max=60, label=list(min=20,max=80, maxVisible = 20)))%&gt;% visOptions(highlightNearest = TRUE, # selectedBy = &quot;group&quot;, # manipulation = TRUE, nodesIdSelection = TRUE) %&gt;% visInteraction(navigationButtons = TRUE) %&gt;% visLegend() %&gt;% visIgraphLayout(layout =&quot;layout.fruchterman.reingold&quot;,smooth = TRUE) net return(net) } 4.4.2 Raw network We can firstly provide a basic view of the network using the default parameters wich visualize all connections greater to zero. geo_network(mod) 4.4.3 Raw network without Tunisia We can eliminate the host country of the media which is obviously present in the majority of news. geo_network(mod %&gt;% filter(i != &quot;TUN&quot;, j !=&quot;TUN&quot;)) 4.4.4 Preferential links But we can also visualize the most significant links by changing the criteria of size and introducing a test of significance : geo_network(mod, size = &quot;Rabs_ij&quot;, minsize = 0.5, test = &quot;Rchi_ij&quot;, mintest = 2.71) "]]
