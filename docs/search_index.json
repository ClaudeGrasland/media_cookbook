<<<<<<< HEAD
[["index.html", "The media cookbook Working paper of the ANR-DFG Imageun Project Chapter 1 Introduction", " The media cookbook Working paper of the ANR-DFG Imageun Project Claude Grasland 2022-07-14 Chapter 1 Introduction The aim of this cookbook is to exchange ideas and programs between members of the “Media Group” of the ANR-DFG project IMAGEUN. "],["11-corpus-mediacloud.html", "Chapter 2 Import from Media Cloud 2.1 Selection of media with source manager 2.2 Checking the stability through time 2.3 Selection of news specifically related to a topic (option) 2.4 Download and storage of news", " Chapter 2 Import from Media Cloud library(quanteda) library(dplyr) library(ggplot2) library(tidytext) library(knitr) knitr::opts_chunk$set(cache = TRUE, echo = TRUE, comment = &quot;&quot;) (tbd : presentation of the MediaCloud project) Mediacloud can be freely used by researchers. All you have to do is to create an account at the following adress : https://explorer.mediacloud.org You have different ways to get title of news. We will focus here on a simple example of data obtained through the mediacloud interface. We suppose that you want to extract news from the Tunisian newspapers speaking from Europe. 2.1 Selection of media with source manager We use the application called Source Manager and we introduce a research by collection which is the most convenient to explore what is available in a country. In our example, the target country is Tunisia and we have three collections that are propsed : We have selected the collection named “Tunisia National” because we are interested in the most important newspapers of the country. The buble graphic on the right indicates immediately the media that has produced the highest number of news, but it is wise to explore in more details the list on the left which indicates for each media the statting date of data collection. When a media appears interesting, we click on its name to obtain a brief summary of the metadata. For example, in the case of L’économiste Maghrebin the metadata indicates : The media looks promising, but before to go further, it can be better to have a look at the website of the media to have a more concrete idea of the content if we don’t know in advance what it is about in terms of content, what is the ideological orientation, etc. Here we can see that this is an ecnomic journal, published in french, with news organized in concentric geographic circles (Nation &gt; Maghreb &gt; Africa &gt; World) which is precisely what we are looking for in the IMAGEUN project. We will further complete the informations about this, but before to do that we have to check in more details if the production of the media is regular through time with another tool offered by mediacloud, the explorer. 2.2 Checking the stability through time We have clicked on search in explorer on the metadata page of the Source Manager and obtain a news interfacce where we modify the date to cover the full period of collection of the media (or our period of interest). In the research field, we let the search term * which indicates a research on all news. Below your request, you obtain a graphic entitled Attention Over Time with the distribution of the number of news published per day which help you to verify if the distribution of news is regular through time. You just have to modify the type of graphic in order to visualize Story Count and you can choose the time span you want (day, week or month) for the evaluation of the regularity of news flow. In our example, we notice that at daily level they are some brief period of break in 2019, but the flow is reasonnabely regular with approximatively 5 news per day at the beginning and 10 to 20 in the final period. We also notice a classical week cycle with a decrease of news published during the week-end. Going down, you will find a news panel entitled Total Attention which gives you the total number of stories found. In our example, we have a total of 13626 stories produced by our media over the period. 2.3 Selection of news specifically related to a topic (option) You can eventually use Mediacloud to check the number of news produced about a specific topic, for example Europe or European Union or EU. The request shouldbe put in lower case with \"\" for compounds. Detailed explanation are available in the query guide. This time you can use the graphic option Stories percentage rather than Story count if you want to viusalize the salience of the topic through time. In our example, we have 369 news that appears to be related to our request about Europe or EU with a relatively regular pattern at month level of 1 to 3 % of news and exceptionaly 5 to 7 %. 2.4 Download and storage of news According to your selection (all news or a specific topic) you will download more or less title. Here, me make the choice to get all news, which means that we have to repeat the original request with *. Finally, by clicking on the button Download all story URLS, you can get a .csv file that you can easily load in your favorite programming language as we will see in the next section. "],["12-mediacloud_to_quanteda.html", "Chapter 3 quanteda corpus 3.1 Example 3.2 Germany 3.3 France 3.4 Royaume-Uni 3.5 Ireland 3.6 Turkey 3.7 Tunisia 3.8 Algeria", " Chapter 3 quanteda corpus library(quanteda) library(dplyr) library(ggplot2) library(tidytext) library(knitr) library(readtext) knitr::opts_chunk$set(cache = TRUE, echo = TRUE, comment = &quot;&quot;) In the previous section (ref…) whe have obtained a .csv file of news collected from MediaCloud. We will try now to put this data in a standard form and we have chosen the format of the quanteda package as reference for data organization and storage. But of course the researchers involved in the project can prefer to use other R packages like tm or tidytext. And they can also prefer to use another programming language for Python. It is the reason why we explain how to transform and export the data that has been prepared and harmonized with quanteda in various format like .csv or JSON. 3.1 Example We detail here an example of importation with the example of the newspaper “L’économiste maghrebin” 3.1.1 Importation of text to R This step is not always obvious because many problems of encoding can appear that are more or less easy to solve. In principle , the data from Media Cloud are exported in standard UTF-8 but as we will see it is not necessary the case. We try firstly to use the standard R function read.csv(): store &lt;- &quot;corpus/TUN/&quot; media &lt;- &quot;fr_TUN_ecomag&quot; type &lt;-&quot;.csv&quot; fic &lt;- paste(store,media,type,sep=&quot;&quot;) df&lt;-read.csv(fic, sep=&quot;,&quot;, header=T, encoding = &quot;UTF-8&quot;, stringsAsFactors = F) head(df) stories_id publish_date 1 1129295780 2019-01-02 03:42:46 2 1129295771 2019-01-02 04:06:27 3 1129295760 2019-01-02 06:05:08 4 1129578051 2019-01-02 10:05:06 5 1129461662 2019-01-02 07:52:36 6 1129461636 2019-01-02 08:57:54 title 1 Les tarifs de l&amp;#8217;ADSL réduits à partir du 1er janvier 2019 2 6ème Sfax Marathon International des Oliviers 3 Télécharger la version finale de la Loi de finances 2019 4 Chawki Tabib : 245 dossiers de corruption présumée transmis au ministère public 5 Panoro Energy finalise l&amp;#8217;acquisition de OMV Tunisia 6 La partie syndicale maintient le boycott des examens du secondaire url 1 https://www.leconomistemaghrebin.com/2019/01/02/tarifs-adsl-reduits-1-janvier-2019/ 2 https://www.leconomistemaghrebin.com/2019/01/02/sfax-marathon-international-oliviers/ 3 https://www.leconomistemaghrebin.com/2019/01/02/telecharger-la-version-finale-de-la-loi-de-finances-2019/ 4 https://www.leconomistemaghrebin.com/2019/01/02/chawki-tabib-245-dossiers-transferes-au-ministere-public/ 5 https://www.leconomistemaghrebin.com/2019/01/02/panoro-energy-finalise-lacquisition-de-omv-tunisia/ 6 https://www.leconomistemaghrebin.com/2019/01/02/partie-syndicale-boycott-examens-secondaire/ language ap_syndicated themes media_id media_name 1 fr False 623820 L&#39;Economiste Maghrebin 2 fr False 623820 L&#39;Economiste Maghrebin 3 en False 623820 L&#39;Economiste Maghrebin 4 fr False 623820 L&#39;Economiste Maghrebin 5 fr False 623820 L&#39;Economiste Maghrebin 6 fr False 623820 L&#39;Economiste Maghrebin media_url 1 http://www.leconomistemaghrebin.com/ 2 http://www.leconomistemaghrebin.com/ 3 http://www.leconomistemaghrebin.com/ 4 http://www.leconomistemaghrebin.com/ 5 http://www.leconomistemaghrebin.com/ 6 http://www.leconomistemaghrebin.com/ str(df) &#39;data.frame&#39;: 12794 obs. of 10 variables: $ stories_id : int 1129295780 1129295771 1129295760 1129578051 1129461662 1129461636 1130259352 1131673651 1132241460 1132432991 ... $ publish_date : chr &quot;2019-01-02 03:42:46&quot; &quot;2019-01-02 04:06:27&quot; &quot;2019-01-02 06:05:08&quot; &quot;2019-01-02 10:05:06&quot; ... $ title : chr &quot;Les tarifs de l&amp;#8217;ADSL réduits à partir du 1er janvier 2019&quot; &quot;6ème Sfax Marathon International des Oliviers&quot; &quot;Télécharger la version finale de la Loi de finances 2019&quot; &quot;Chawki Tabib : 245 dossiers de corruption présumée transmis au ministère public&quot; ... $ url : chr &quot;https://www.leconomistemaghrebin.com/2019/01/02/tarifs-adsl-reduits-1-janvier-2019/&quot; &quot;https://www.leconomistemaghrebin.com/2019/01/02/sfax-marathon-international-oliviers/&quot; &quot;https://www.leconomistemaghrebin.com/2019/01/02/telecharger-la-version-finale-de-la-loi-de-finances-2019/&quot; &quot;https://www.leconomistemaghrebin.com/2019/01/02/chawki-tabib-245-dossiers-transferes-au-ministere-public/&quot; ... $ language : chr &quot;fr&quot; &quot;fr&quot; &quot;en&quot; &quot;fr&quot; ... $ ap_syndicated: chr &quot;False&quot; &quot;False&quot; &quot;False&quot; &quot;False&quot; ... $ themes : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... $ media_id : int 623820 623820 623820 623820 623820 623820 623820 623820 623820 623820 ... $ media_name : chr &quot;L&#39;Economiste Maghrebin&quot; &quot;L&#39;Economiste Maghrebin&quot; &quot;L&#39;Economiste Maghrebin&quot; &quot;L&#39;Economiste Maghrebin&quot; ... $ media_url : chr &quot;http://www.leconomistemaghrebin.com/&quot; &quot;http://www.leconomistemaghrebin.com/&quot; &quot;http://www.leconomistemaghrebin.com/&quot; &quot;http://www.leconomistemaghrebin.com/&quot; ... The importation was successfull for 13623 news but message of errors appeared for 3 news where R sent a message of error telling : Error in gregexpr(calltext, singleline, fixed = TRUE) : regular expression is invalid UTF-8 Looking in more details, we discover also some problems of encoding in news like in the following example where the text of the news appears differently if we apply the standard functions paste() o0 the specialized function r knitr::kable for printing. paste(df[9, 3]) [1] &quot;Néji Jalloul : &amp;#8220;Nidaa Tounes peut revenir si&amp;#8230;&amp;#8221;&quot; kable((df[9,3])) x Néji Jalloul : “Nidaa Tounes peut revenir si…” 3.1.2 encoding problems It is sometime possible to adapt manually the encoding problem whan they are not too much as in present example. df$text&lt;-df$title # standardize apostrophe df$text&lt;-gsub(&quot;&amp;#8217;&quot;,&quot;&#39;&quot;,df$text) # standardize punct df$text&lt;-gsub(&#39;&amp;#8230;&#39;,&#39;.&#39;,df$text) # standardize hyphens df$text&lt;-gsub(&#39;&amp;#8211;&#39;,&#39;-&#39;,df$text) # Remove quotation marks df$text&lt;-gsub(&#39;&amp;#171;&amp;#160;&#39;,&#39;&#39;,df$text) df$text&lt;-gsub(&#39;&amp;#160;&amp;#187;&#39;,&#39;&#39;,df$text) df$text&lt;-gsub(&#39;&amp;#8220;&#39;,&#39;&#39;,df$text) df$text&lt;-gsub(&#39;&amp;#8221;&#39;,&#39;&#39;,df$text) df$text&lt;-gsub(&#39;&amp;#8216;&#39;,&#39;&#39;,df$text) df$text&lt;-gsub(&#39;&amp;#8243;&#39;,&#39;&#39;,df$text) We can introduce othr cleaning procedures here or keep it for later analysis 3.1.3 Transformation in quanteda format We propose a storage based on quanteda format by just transforming the data that has been produced by readtext. We keep only the name of the source and the date of publication. # Create Quanteda corpus qd&lt;-corpus(df,docid_field = &quot;stories_id&quot;) # Select docvar fields and rename media qd$date &lt;-as.Date(qd$publish_date) qd$source &lt;-media docvars(qd)&lt;-docvars(qd)[,c(&quot;source&quot;,&quot;date&quot;)] # Add global meta meta(qd,&quot;meta_source&quot;)&lt;-&quot;Media Cloud &quot; meta(qd,&quot;meta_time&quot;)&lt;-&quot;Download the 2021-09-30&quot; meta(qd,&quot;meta_author&quot;)&lt;-&quot;Elaborated by Claude Grasland&quot; meta(qd,&quot;project&quot;)&lt;-&quot;ANR-DFG Project IMAGEUN&quot; We have created a quanteda object with a lot of information stored in various fields. The structure of the object is the following one str(qd) &#39;corpus&#39; Named chr [1:12794] &quot;Les tarifs de l&#39;ADSL réduits à partir du 1er janvier 2019&quot; ... - attr(*, &quot;names&quot;)= chr [1:12794] &quot;1129295780&quot; &quot;1129295771&quot; &quot;1129295760&quot; &quot;1129578051&quot; ... - attr(*, &quot;docvars&quot;)=&#39;data.frame&#39;: 12794 obs. of 5 variables: ..$ docname_: chr [1:12794] &quot;1129295780&quot; &quot;1129295771&quot; &quot;1129295760&quot; &quot;1129578051&quot; ... ..$ docid_ : Factor w/ 12794 levels &quot;1129295780&quot;,&quot;1129295771&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ..$ segid_ : int [1:12794] 1 1 1 1 1 1 1 1 1 1 ... ..$ source : chr [1:12794] &quot;fr_TUN_ecomag&quot; &quot;fr_TUN_ecomag&quot; &quot;fr_TUN_ecomag&quot; &quot;fr_TUN_ecomag&quot; ... ..$ date : Date[1:12794], format: &quot;2019-01-02&quot; &quot;2019-01-02&quot; ... - attr(*, &quot;meta&quot;)=List of 3 ..$ system:List of 6 .. ..$ package-version:Classes &#39;package_version&#39;, &#39;numeric_version&#39; hidden list of 1 .. .. ..$ : int [1:3] 3 0 0 .. ..$ r-version :Classes &#39;R_system_version&#39;, &#39;package_version&#39;, &#39;numeric_version&#39; hidden list of 1 .. .. ..$ : int [1:3] 4 0 2 .. ..$ system : Named chr [1:3] &quot;Darwin&quot; &quot;x86_64&quot; &quot;claudegrasland1&quot; .. .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;sysname&quot; &quot;machine&quot; &quot;user&quot; .. ..$ directory : chr &quot;/Users/claudegrasland1/git/media_cookbook&quot; .. ..$ created : Date[1:1], format: &quot;2022-07-12&quot; .. ..$ source : chr &quot;data.frame&quot; ..$ object:List of 2 .. ..$ unit : chr &quot;documents&quot; .. ..$ summary:List of 2 .. .. ..$ hash: chr(0) .. .. ..$ data: NULL ..$ user :List of 4 .. ..$ meta_source: chr &quot;Media Cloud &quot; .. ..$ meta_time : chr &quot;Download the 2021-09-30&quot; .. ..$ meta_author: chr &quot;Elaborated by Claude Grasland&quot; .. ..$ project : chr &quot;ANR-DFG Project IMAGEUN&quot; We can look at the first titles with head() kable(head(qd,3)) x 1129295780 Les tarifs de l’ADSL réduits à partir du 1er janvier 2019 1129295771 6ème Sfax Marathon International des Oliviers 1129295760 Télécharger la version finale de la Loi de finances 2019 We can get meta information on each stories with summary() summary(head(qd,3)) Corpus consisting of 3 documents, showing 3 documents: Text Types Tokens Sentences source date 1129295780 11 11 1 fr_TUN_ecomag 2019-01-02 1129295771 6 6 1 fr_TUN_ecomag 2019-01-02 1129295760 8 10 1 fr_TUN_ecomag 2019-01-02 We can get meta information about the full document meta(qd) $meta_source [1] &quot;Media Cloud &quot; $meta_time [1] &quot;Download the 2021-09-30&quot; $meta_author [1] &quot;Elaborated by Claude Grasland&quot; $project [1] &quot;ANR-DFG Project IMAGEUN&quot; 3.1.4 Storage of the quanteda object We can finally save the object in .RDS format in a directory dedicated to our quanteda files. It can be usefull to give some information in the name of the file store &lt;- &quot;quanteda/&quot; type&lt;- &quot;.RDS&quot; myfile &lt;- paste(store,media,type,sep=&quot;&quot;) myfile [1] &quot;quanteda/fr_TUN_ecomag.RDS&quot; saveRDS(qd,myfile) qd[1:3] Corpus consisting of 3 documents and 2 docvars. 1129295780 : &quot;Les tarifs de l&#39;ADSL réduits à partir du 1er janvier 2019&quot; 1129295771 : &quot;6ème Sfax Marathon International des Oliviers&quot; 1129295760 : &quot;Télécharger la version finale de la Loi de finances 2019&quot; summary(qd,3) Corpus consisting of 12794 documents, showing 3 documents: Text Types Tokens Sentences source date 1129295780 11 11 1 fr_TUN_ecomag 2019-01-02 1129295771 6 6 1 fr_TUN_ecomag 2019-01-02 1129295760 8 10 1 fr_TUN_ecomag 2019-01-02 We have kept all the information present in the initial file, but also added specific metadata of interest for us. The size of the storage is now equal to 0.6 Mb which means a division by 6 as compared to the initial .csv file downloaded from Media Cloud. 3.1.5 Backtransformation of quanteda to data.table or tibble In the following steps, we will make an intensive use of quanteda, but sometimes it can be usefull to export the results in a more practical format or to use other packages. For this reasons, it is important to know that the tidytextpackage can easily transform quanteda object in tibbles which are more classical and easy to manage td &lt;- tidy(qd) Warning: &#39;texts.corpus&#39; is deprecated. Use &#39;as.character&#39; instead. See help(&quot;Deprecated&quot;) head(td) # A tibble: 6 × 3 text source date &lt;chr&gt; &lt;chr&gt; &lt;date&gt; 1 Les tarifs de l&#39;ADSL réduits à partir du 1er janvier 2019 fr_TU… 2019-01-02 2 6ème Sfax Marathon International des Oliviers fr_TU… 2019-01-02 3 Télécharger la version finale de la Loi de finances 2019 fr_TU… 2019-01-02 4 Chawki Tabib : 245 dossiers de corruption présumée transmis… fr_TU… 2019-01-02 5 Panoro Energy finalise l&#39;acquisition de OMV Tunisia fr_TU… 2019-01-02 6 La partie syndicale maintient le boycott des examens du sec… fr_TU… 2019-01-02 str(td) tibble [12,794 × 3] (S3: tbl_df/tbl/data.frame) $ text : chr [1:12794] &quot;Les tarifs de l&#39;ADSL réduits à partir du 1er janvier 2019&quot; &quot;6ème Sfax Marathon International des Oliviers&quot; &quot;Télécharger la version finale de la Loi de finances 2019&quot; &quot;Chawki Tabib : 245 dossiers de corruption présumée transmis au ministère public&quot; ... $ source: chr [1:12794] &quot;fr_TUN_ecomag&quot; &quot;fr_TUN_ecomag&quot; &quot;fr_TUN_ecomag&quot; &quot;fr_TUN_ecomag&quot; ... $ date : Date[1:12794], format: &quot;2019-01-02&quot; &quot;2019-01-02&quot; ... 3.2 Germany 3.2.1 Frankfurter Allgemeine Zeitung qd&lt;-readRDS(&quot;quanteda/de_DEU_frankf.RDS&quot;) summary(qd,3) Corpus consisting of 88313 documents, showing 3 documents: Text Types Tokens Sentences source date regs nbregs 930016745 5 5 1 de_DEU_frankf 2020-09-06 0 1118202030 10 10 1 de_DEU_frankf 2021-07-06 0 1128764967 14 14 1 de_DEU_frankf 2019-01-01 Q1286 1 paste(qd[1:5]) [1] &quot;Bundestagswahl: Durchmarsch der AfD&quot; [2] &quot;Kahl folgt auf Schindler: CSU kritisiert Wechsel an BND-Spitze&quot; [3] &quot;Dramatisches Ende einer Bergtour: Drei Deutsche nach eisiger Nacht in Tiroler Alpen gerettet&quot; [4] &quot;Trotz Gelbwesten-Protesten: Macron kündigt für 2019 weitere Reformen an&quot; [5] &quot;May wirbt in Neujahrsansprache für Brexit-Abkommen&quot; 3.2.2 Süddeutsche Zeitung qd&lt;-readRDS(&quot;quanteda/de_DEU_suddeu.RDS&quot;) summary(qd,5) Corpus consisting of 35444 documents, showing 5 documents: Text Types Tokens Sentences source date regs nbregs 931884497 4 4 1 de_DEU_suddeu 2019-06-17 Q458 1 1128842071 8 8 1 de_DEU_suddeu 2019-01-01 0 1128842047 8 8 1 de_DEU_suddeu 2019-01-01 0 1128842007 4 4 1 de_DEU_suddeu 2019-01-01 0 1128842024 5 5 1 de_DEU_suddeu 2019-01-01 0 paste(qd[1:5]) [1] &quot;EU setzt Russland Ultimatum&quot; [2] &quot;Flüchtlinge : Großbritannien verlegt Schiffe in den Ärmelkanal&quot; [3] &quot;Zugang zum Internet : Buena Vista Netz Club&quot; [4] &quot;Brasilien: Bolsonaros Plan&quot; [5] &quot;Dirty Dancing: Getanzter Dreck&quot; 3.3 France 3.3.1 Le Figaro qd&lt;-readRDS(&quot;quanteda/fr_FRA_figaro.RDS&quot;) summary(qd,5) Corpus consisting of 128807 documents, showing 5 documents: Text Types Tokens Sentences source date regs nbregs 1128863317 12 12 1 fr_FRA_figaro 2019-01-01 ST_DEU 1 1128915698 11 11 1 fr_FRA_figaro 2019-01-01 CA_JPN 1 1128915693 8 8 1 fr_FRA_figaro 2019-01-01 ST_THA 1 1128915594 8 8 1 fr_FRA_figaro 2019-01-01 0 1128915683 8 8 1 fr_FRA_figaro 2019-01-01 0 tags nbtags 0 CA_JPN 1 ST_THA 1 0 0 paste(qd[1:5]) [1] &quot;Légion d&#39;honneur : autant de femmes distinguées que d&#39;hommes ce 1er janvier&quot; [2] &quot;Tokyo : une voiture fonce dans la foule, 9 blessés&quot; [3] &quot;Thaïlande : le roi sera couronné en mai&quot; [4] &quot;Ces joueurs qui pourraient enflammer le mercato hivernal&quot; [5] &quot;Strasbourg : 9 blessés à cause des pétards&quot; 3.3.2 Le Monde qd&lt;-readRDS(&quot;quanteda/fr_FRA_lmonde.RDS&quot;) summary(qd,5) Corpus consisting of 49731 documents, showing 5 documents: Text Types Tokens Sentences source date regs nbregs 1128825289 12 12 1 fr_FRA_lmonde 2019-01-01 0 1128864671 11 11 1 fr_FRA_lmonde 2019-01-01 0 1128864634 9 9 1 fr_FRA_lmonde 2019-01-01 0 1128864655 13 13 1 fr_FRA_lmonde 2019-01-01 0 1128908122 11 11 1 fr_FRA_lmonde 2019-01-01 0 paste(qd[1:5]) [1] &quot;Les pesticides de synthèse interdits aux particuliers à partir du 1er janvier&quot; [2] &quot;Mauritanie : un député antiesclavagiste libéré après cinq mois de prison&quot; [3] &quot;Jeanne Calment était-elle vraiment la doyenne de l’humanité ?&quot; [4] &quot;Italie : après l’adoption du budget, 2019 commence dans une ambiance chaotique&quot; [5] &quot;Pérou : destitution de deux procureurs qui enquêtaient sur l’affaire Odebrecht&quot; 3.4 Royaume-Uni 3.4.1 The Guardian qd&lt;-readRDS(&quot;quanteda/en_GBR_guardi.RDS&quot;) summary(qd,5) Corpus consisting of 77875 documents, showing 5 documents: Text Types Tokens Sentences source date regs nbregs 1128777161 12 12 1 en_GBR_guardi 2019-01-01 Q15 1 1128777148 10 10 1 en_GBR_guardi 2019-01-01 Q7204 1 1128795880 11 11 1 en_GBR_guardi 2019-01-01 0 1128795872 9 9 1 en_GBR_guardi 2019-01-01 Q15 1 1128795860 12 13 1 en_GBR_guardi 2019-01-01 0 paste(qd[1:5]) [1] &quot;Toxic legacy taints ANC as it nears 25-year rule in South Africa&quot; [2] &quot;Why Trump’s Middle East peace plan is just a sideshow&quot; [3] &quot;Preparations for the Harbin ice and snow festival – in pictures&quot; [4] &quot;Turning air into drinking water: Africa&#39;s inspired inventors&quot; [5] &quot;&#39;Resign from Facebook&#39;: experts offer Mark Zuckerberg advice for 2019&quot; 3.4.2 The Daily Telegraph qd&lt;-readRDS(&quot;quanteda/en_GBR_telegr.RDS&quot;) summary(qd,5) Corpus consisting of 41803 documents, showing 5 documents: Text Types Tokens Sentences source date regs nbregs 1001621055 10 13 2 en_GBR_telegr 2020-03-15 0 794012047 6 7 1 en_GBR_telegr 2021-04-15 0 1020025042 11 11 1 en_GBR_telegr 2020-07-06 Q46 1 1071998286 7 7 1 en_GBR_telegr 2021-06-26 0 1080057597 11 12 1 en_GBR_telegr 2020-06-15 0 paste(qd[1:5]) [1] &quot;Guant?namo &#39;suicides&#39; were at secret &#39;black&#39; site&quot; [2] &quot;IKEA founder &#39;was Nazi recruiter&#39;&quot; [3] &quot;Europe tells Britain to justify itself over fingerprinting children in schools&quot; [4] &quot;Sumo champion Asashoryu outrages sport with celebration&quot; [5] &quot;Barack Obama meets Jewish leaders over &#39;tough line&#39; on Israel&quot; 3.5 Ireland 3.5.1 The Irish Time qd&lt;-readRDS(&quot;quanteda/en_IRL_irtime.RDS&quot;) summary(qd,5) Corpus consisting of 108479 documents, showing 5 documents: Text Types Tokens Sentences source date regs nbregs 1003842232 8 9 1 en_IRL_irtime 2020-01-15 0 1128791232 8 8 1 en_IRL_irtime 2019-01-01 0 1128791226 7 7 1 en_IRL_irtime 2019-01-01 0 1128803852 10 10 1 en_IRL_irtime 2019-01-01 0 1128874798 11 11 1 en_IRL_irtime 2019-01-01 0 paste(qd[1:5]) [1] &quot;Strong corporate tax receipts ‘sustainable’ until 2020&quot; [2] &quot;Number of British people becoming Irish citizens surges&quot; [3] &quot;7 tips to build your physical stamina&quot; [4] &quot;Having a baby abroad changed my perspective on living away&quot; [5] &quot;New Year’s Eve celebrations around the world to ring in 2019&quot; 3.5.2 The Belfast Telegraph qd&lt;-readRDS(&quot;quanteda/en_NIR_beltel.RDS&quot;) summary(qd,5) Corpus consisting of 99932 documents, showing 5 documents: Text Types Tokens Sentences source date regs nbregs 1128845104 8 8 1 en_NIR_beltel 2019-01-01 0 1128845083 11 11 1 en_NIR_beltel 2019-01-01 0 1128845064 9 9 1 en_NIR_beltel 2019-01-01 0 1128845042 11 11 1 en_NIR_beltel 2019-01-01 0 1128905097 10 10 1 en_NIR_beltel 2019-01-01 0 paste(qd[1:5]) [1] &quot;Officer disarmed knifeman in Foyleside Centre court told&quot; [2] &quot;Hoaxers are putting lives at risk, warns fire service officer&quot; [3] &quot;The volunteers providing a lifeline to elderly Belfast residents&quot; [4] &quot;Tributes to Northern Ireland man Christopher Donnelly after death in US&quot; [5] &quot;Belfast bar to build rooftop terrace in £350k extension&quot; 3.6 Turkey 3.6.1 The Yeni Sati qd&lt;-readRDS(&quot;quanteda/tr_TUR_yenisa.RDS&quot;) summary(qd,5) Corpus consisting of 97638 documents, showing 5 documents: Text Types Tokens Sentences source date regs nbregs 1128735746 4 4 1 tr_TUR_yenisa 2019-01-01 0 1128760707 5 5 1 tr_TUR_yenisa 2019-01-01 0 1128776505 7 7 1 tr_TUR_yenisa 2019-01-01 0 1128776514 7 7 1 tr_TUR_yenisa 2019-01-01 0 1128796514 4 4 1 tr_TUR_yenisa 2019-01-01 0 paste(qd[1:5]) [1] &quot;Bingöl&#39;de 4.2 şiddetinde deprem&quot; [2] &quot;Pentagon&#39;da deprem: Görevden alındı&quot; [3] &quot;Cumhurbaşkanı Erdoğan en seçkin dünya lideri seçildi&quot; [4] &quot;Japonya&#39;da araçlı saldırı: Yayaların arasında daldı&quot; [5] &quot;Almanya&#39;da Mekke&#39;nin fethi kutlandı&quot; 3.6.2 Cumhuryet qd&lt;-readRDS(&quot;quanteda/tr_TUR_cumhur.RDS&quot;) summary(qd,5) Corpus consisting of 133713 documents, showing 5 documents: Text Types Tokens Sentences source date regs nbregs 1129020777 7 7 1 tr_TUR_cumhur 2019-01-01 0 1129020771 8 8 1 tr_TUR_cumhur 2019-01-01 0 1129020761 5 5 1 tr_TUR_cumhur 2019-01-01 0 1129000336 6 6 1 tr_TUR_cumhur 2019-01-01 0 1129020744 4 4 1 tr_TUR_cumhur 2019-01-01 0 paste(qd[1:5]) [1] &quot;Hakkari-Şırnak kara yolunda çığ düşmesi sonucu kapandı&quot; [2] &quot;Adana&#39;da 10 araç birbirine girdi: Yaralılar var&quot; [3] &quot;CHP binasında partililerden oturma eylemi&quot; [4] &quot;Suriye sınırına askeri sevkiyat devam ediyor&quot; [5] &quot;İsveç&#39;te camiye silahlı saldırı&quot; 3.7 Tunisia 3.7.1 Réalités qd&lt;-readRDS(&quot;quanteda/fr_TUN_realit.RDS&quot;) summary(qd,5) Corpus consisting of 23042 documents, showing 5 documents: Text Types Tokens Sentences source date regs nbregs 1128872823 8 8 1 fr_TUN_realit 2019-01-01 0 1128936210 11 11 1 fr_TUN_realit 2019-01-01 0 1128961861 12 12 1 fr_TUN_realit 2019-01-01 0 1129004162 10 10 1 fr_TUN_realit 2019-01-01 0 1129004150 5 5 1 fr_TUN_realit 2019-01-01 0 paste(qd[1:5]) [1] &quot;L&#39;Est de la Turquie frappé par un séisme&quot; [2] &quot;Enfidha : un Omda auteur d&#39;un trafic de logements sociaux ?&quot; [3] &quot;Russie : la mort d&#39;un tunisien en prison suscite encore des interrogations&quot; [4] &quot;Abattage de vaches malades à Sousse: Ouverture d&#39;une enquête&quot; [5] &quot;Quels remèdes aux incertitudes ?&quot; 3.7.2 L’économiste Maghrebin qd&lt;-readRDS(&quot;quanteda/fr_TUN_ecomag.RDS&quot;) summary(qd,5) Corpus consisting of 12794 documents, showing 5 documents: Text Types Tokens Sentences source date 1129295780 11 11 1 fr_TUN_ecomag 2019-01-02 1129295771 6 6 1 fr_TUN_ecomag 2019-01-02 1129295760 8 10 1 fr_TUN_ecomag 2019-01-02 1129578051 12 12 1 fr_TUN_ecomag 2019-01-02 1129461662 7 7 1 fr_TUN_ecomag 2019-01-02 paste(qd[1:5]) [1] &quot;Les tarifs de l&#39;ADSL réduits à partir du 1er janvier 2019&quot; [2] &quot;6ème Sfax Marathon International des Oliviers&quot; [3] &quot;Télécharger la version finale de la Loi de finances 2019&quot; [4] &quot;Chawki Tabib : 245 dossiers de corruption présumée transmis au ministère public&quot; [5] &quot;Panoro Energy finalise l&#39;acquisition de OMV Tunisia&quot; 3.7.3 La Presse qd&lt;-readRDS(&quot;quanteda/fr_TUN_presse.RDS&quot;) summary(qd,5) Corpus consisting of 11553 documents, showing 5 documents: Text Types Tokens Sentences source date regs nbregs 1319140184 16 21 1 fr_TUN_presse 2019-04-24 0 1332556981 6 6 1 fr_TUN_presse 2019-07-08 0 1378389487 11 12 1 fr_TUN_presse 2019-08-29 0 1378389531 21 24 1 fr_TUN_presse 2019-08-29 0 1378389503 12 12 1 fr_TUN_presse 2019-08-29 0 paste(qd[1:5]) [1] &quot;Grève de 24 heures à l&#39;aéroport de Tunis Carthage à partir de ce soir à minuit | La Presse de Tunisie&quot; [2] &quot;Accueil | La Presse de Tunisie&quot; [3] &quot;Nomination de Najet Neili, coordinatrice des Journées cinématographiques de Carthage 2019&quot; [4] &quot;40ème jour du décès du président de la République Béji Caïd Essebsi : « On fait tout pour empêcher l’organisation de la cérémonie »&quot; [5] &quot;Iyadh Elloumi : « Nous entamerons une grève de la faim »&quot; 3.7.4 Babnet qd&lt;-readRDS(&quot;quanteda/ar_TUN_babnet.RDS&quot;) summary(qd,5) Corpus consisting of 42649 documents, showing 5 documents: Text Types Tokens Sentences source date regs nbregs 1128915717 17 17 1 ar_TUN_babnet 2019-01-01 0 1128915704 13 13 1 ar_TUN_babnet 2019-01-01 0 1128915643 11 11 1 ar_TUN_babnet 2019-01-01 0 1128915651 16 16 1 ar_TUN_babnet 2019-01-01 0 1128915618 6 6 1 ar_TUN_babnet 2019-01-01 0 paste(qd[1:5]) [1] &quot;سعد الحريري: على كل القوى السياسية أن تتنازل لتشكيل الحكومة وواثق من أننا سنصل إلى حل &quot; [2] &quot;مقتل أكثر من 300 شخص في حوادث سير بتايلاند خلال احتفالات العام الجديد &quot; [3] &quot; المرزوقي يطالب التونسيين بالكثير من العمل ويدعوهم للكفّ عن الشتم والخصام &quot; [4] &quot; مع انطلاق سنة 2019: البنك المركزي التونسي يسمح لمؤسسات الدفع الانطلاق في اسداء خدماتها للعملاء&quot; [5] &quot;Seoul Welcomes Kim&#39;s New Year Address &quot; 3.8 Algeria 3.8.1 Al Nahar qd&lt;-readRDS(&quot;quanteda/ar_DZA_alnaha.RDS&quot;) summary(qd,5) Corpus consisting of 36341 documents, showing 5 documents: Text Types Tokens Sentences source date regs nbregs 1128917505 4 4 1 ar_DZA_alnaha 2019-01-01 0 1128917491 7 7 1 ar_DZA_alnaha 2019-01-01 0 1128917468 7 7 1 ar_DZA_alnaha 2019-01-01 0 1128917411 5 5 1 ar_DZA_alnaha 2019-01-01 Q15 1 1128917432 5 5 1 ar_DZA_alnaha 2019-01-01 0 paste(qd[1:5]) [1] &quot;غول يستبعد تأجيل الرئاسيات&quot; [2] &quot;حصيلة الجيش لمختلف العمليات المنفذة في 2018&quot; [3] &quot;الكاف تُسقط محرز كأفضل لاعب إفريقي!&quot; [4] &quot;صلاح يتفوق على محرز إفريقيا &quot; [5] &quot;اتحاد بلعباس في مفترق الطرق &quot; 3.8.2 El Kahber qd&lt;-readRDS(&quot;quanteda/ar_DZA_elkahb.RDS&quot;) summary(qd,5) Corpus consisting of 57230 documents, showing 5 documents: Text Types Tokens Sentences source date regs nbregs 1128760341 10 10 1 ar_DZA_elkahb 2019-01-01 0 1128777970 11 12 2 ar_DZA_elkahb 2019-01-01 0 1128790442 7 7 1 ar_DZA_elkahb 2019-01-01 0 1128804378 11 11 1 ar_DZA_elkahb 2019-01-01 0 1128804373 8 8 1 ar_DZA_elkahb 2019-01-01 0 paste(qd[1:5]) [1] &quot;وهران: تأكيد حكم الإعدام لقاتل الطفلة سلسبيل و شريكه&quot; [2] &quot;بونجاح.. الجزائري الوحيد في التشكيلة المثالية للقارة السمراء لـ2018 !&quot; [3] &quot;إدارة فنربخشة توافق على طلب سليماني !&quot; [4] &quot;وفاة مهندسة في ورشة عدل ببوينان بعد سقوط مواد بناء عليها&quot; [5] &quot;إقالة المتحدثة باسم البنتاغون بسبب معاملتها السيئة للموظفين&quot; 3.8.3 El Watan qd&lt;-readRDS(&quot;quanteda/fr_DZA_elwata.RDS&quot;) summary(qd,5) Corpus consisting of 9898 documents, showing 5 documents: Text Types Tokens Sentences source date regs nbregs 1223806019 4 4 1 fr_DZA_elwata 2019-03-14 0 1223806008 8 8 1 fr_DZA_elwata 2019-03-14 0 1223806016 12 12 1 fr_DZA_elwata 2019-03-14 0 1223806005 8 9 1 fr_DZA_elwata 2019-03-14 0 1223805998 11 11 1 fr_DZA_elwata 2019-03-14 0 paste(qd[1:5]) [1] &quot;On vous le dit&quot; [2] &quot;Un jeune de 17 ans retrouvé à Ghardaïa&quot; [3] &quot;Agence Cnas du port d’Alger : Une demi-journée pour une signature…&quot; [4] &quot;Saisie de 600 kg de kif traité à Khenchela&quot; [5] &quot;Mascara : La cour confirme la peine de prison pour Gharmoul&quot; "],["21-Wikidata.html", "Chapter 4 Wikidata 4.1 Introduction 4.2 Wikidata 4.3 The package WikidataR", " Chapter 4 Wikidata library(knitr) library(dplyr) library(WikidataR) library(quanteda) Authors : Claude Grasland &amp; Etienne Toureille 4.1 Introduction The objective of thisdocument is to explore the possibility of Wikidata for the production of multilingual dictionaries of world regions and more generally regional imaginations. In order to test the interest of this approach, we will try to produce multilingual dictionaries for the identification of five groups of entities : Europe and its subregions Africa and its subregions Asia and its subregions Mediterranea Middle East, Near East, Persian Gulf … The dictionary will be established in five languages of interest for the project IMAGEUN : english : applied to media of UK and Ireland french : applied to media of France and Tunisia german : applied to media of Germany turkish : applied to media of Turkey arabic : applied to media of Tunisia 4.2 Wikidata Wikidata defines itself as a free and open knowledge base that can be read and edited by both humans and machines. as central storage for the structured data of its Wikimedia sister projects including Wikipedia, Wikivoyage, Wiktionary, Wikisource, and others. a support to many other sites and services beyond just Wikimedia projects! The content of Wikidata is available under a free license, exported using standard formats, and can be interlinked to other open data sets on the linked data web. 4.2.1 Codification of entities The first interest of wikidata is to provide unique code of identifications of objects. For example a research about “Africa” will produce a list of different objects characterized by a unique code : knitr::include_graphics(&quot;pics/Wikidata001.png&quot;) 4.2.2 Informations on entities Once we have selected an entity (e.g. Q15) we obtain a new page with more detailed informations in english but also in all other languages available in Wikipedia. knitr::include_graphics(&quot;pics/Wikidata002.png&quot;) A lot of information are available concerning the entity but, at this stage, the most important ones for our research are : the translation in different languages the equivalent words or expression in different languages the definitions in different languages the ambiguity of the term in each language and the potential risks of confusion with other entities. Of course we should not take for granted the answers proposed by wikidata (as noticed by Georg, Wikipedia is a matter of research for IMAGEUN …) but without any doubt, it offers a very good opportunity to clarify our questions and help us to build tools for recognition of world regions and other geographical imaginations in a multilingual perspective. 4.2.3 Wikipedia entities as nodes of an ontolongy It appears crucial to introduce here a clear distinction between Wikipedia entities and textual units associated to the names and definiton of this units. A wikipedia entity like Q15 is an element of an ontology designed by its author for specific purposes. The specificity of the wikidata ontology is the fact that it is a multilinligual web where Q15 is a node of the web present in different linguistic layers. It means that we don’t have a single name or a single definition of Q15, except if we adopt the neocolonial perspective to choose the english language as reference. Depending on the context (i.e. the language or sub-language), Q15 could be defined as : (fr) : A “continent” named “Afrique”\" (en) : A “continent on the Earth’s northern and southern hemispheres” named “Africa” or “African continent” (de) : A “Kontinent auf der Nord- und Südhalbkugel der Erde” named “Afrika” (tr) : A “Dünya’nın kuzey ve güney yarıkürelerindeki bir kıta” named “Afrika” or “Afrika kıtası” (ar) : The \" ثاني أكبر قارات العالم من حيث المساحة وعدد السكان، تأتي في المرتبة الثانية بعد آسيا\" 1 named “إفريقيا” or “القارة الأفريقية” In other words the existence of the same code of wikipedia entities does not offer any guarantee of concordance between the geographical objects found in news published in different languages or different countries. But - and it is the important point - it help us to point similarities and differences between set of geographical entities that are more or less comparable in each language. 4.2.4 A tool for cross-linguistical experiments Having in mind the limits of the equivalence of entities across languages, it can nevertheless be an interesting experience to select a set of wikipedia entities (Q15, Q258, Q4412 …) and to examine their relative frequency in our different media from different countries with different languages. A typical hypothesis could be something like : Is Q15 more mentionned than Q46 in Tunisian newspapers ? which is not equivalent to the question Is Africa more mentionned than Europe in Tunisian newspapers but rather equivalent to the two joint questions Is the textual unit “Afrique” more mentionned than the textual unit “Europe” in Tunisian newspapers published in french language. Is the textual unit “إفريقيا” more mentionned than the textual unit “أوروبا” in Tunisian newspapers published in arabic language. 4.3 The package WikidataR The package WikidataR is an interface for the use of the Wikidata API in R language. Equivalent tools are available in Python and other languages for those non familiar with R. And it is of course possible to use directly the API. The first step is to install the most recent version of the R package WikidataR which install also related packages of interest. #install.packages(&quot;WikidataR&quot;) library(WikidataR) 4.3.1 Basic operations (based on Etienne Toureille previous experiments) 4.3.1.1 identification of entities of interest The function find_item will help to find all wikipedia entities (=items) associated to a textual unit (word or group of word) in given language. Let’s start with the research of entities associated to “Afrique” in french language : mytext &lt;- &quot;Afrique&quot; items &lt;- find_item(search_term = mytext, language = &quot;fr&quot;, limit=30) class(items) [1] &quot;find_item&quot; length(items) [1] 30 The resulting object is an object from type find_item which is in practice a list describing the entities that has been recognized associated to the textual unit that we have chosen. In the french cas, we have found 50 entities that match with our textual unit. Let’s have a look at the first one : items[[1]] $id [1] &quot;Q15&quot; $title [1] &quot;Q15&quot; $pageid [1] 111 $display $display$label $display$label$value [1] &quot;Africa&quot; $display$label$language [1] &quot;en&quot; $display$description $display$description$value [1] &quot;continent on the Earth&#39;s northern and southern hemispheres&quot; $display$description$language [1] &quot;en&quot; $repository [1] &quot;wikidata&quot; $url [1] &quot;//www.wikidata.org/wiki/Q15&quot; $concepturi [1] &quot;http://www.wikidata.org/entity/Q15&quot; $label [1] &quot;Africa&quot; $description [1] &quot;continent on the Earth&#39;s northern and southern hemispheres&quot; $match $match$type [1] &quot;label&quot; $match$language [1] &quot;fr&quot; $match$text [1] &quot;Afrique&quot; $aliases $aliases[[1]] [1] &quot;Afrique&quot; As we can see we can easily identify the code the label and description in english but also the text responsible from the matching answer in french. We can therefore create a function item_info that extract all elements of interest and put them in a table in order to have a complete view. item_info &lt;- function(my_item){ if (is.null(my_item$id) == F){item_id = my_item$id} else {item_id = NA} if (is.null(my_item$label) ==F){item_label = my_item$label} else {item_label = NA} if (is.null(my_item$desc) == F) {item_desc= my_item$desc} else {item_desc = NA} if (is.null(my_item$match$lang) ==F){item_lang = my_item$match$lang} else {item_lang = NA} if (is.null(my_item$match$text) ==F){item_text = my_item$match$text} else {item_text = NA} res&lt;-data.frame(item_id,item_label,item_desc,item_lang,item_text) return(res) } For example : item_info(items[[1]]) item_id item_label item_desc 1 Q15 Africa continent on the Earth&#39;s northern and southern hemispheres item_lang item_text 1 fr Afrique We build then a second function that extract all the wikipedia entities associated to a textual unit for a given language extract_entities &lt;- function(mytext= &quot;Afrique&quot;, mylang = &quot;fr&quot;, maxres = 20) { # Extract items items &lt;- find_item(search_term = mytext, language = mylang, limit = maxres) # Create empty dataset res&lt;-data.frame() res$item_id &lt;- as.character() res$item_label &lt;- as.character() res$item_desc &lt;- as.character() res$item_lang &lt;- as.character() res$item_text &lt;- as.character() # Fill dataset k&lt;-length(items) for (i in 1:k) { res &lt;- rbind(res,item_info(items[[i]])) } # Return dataset return(res) } For example : tab &lt;- extract_entities(&quot;Afrique&quot;,&quot;fr&quot;,20) kable(tab) item_id item_label item_desc item_lang item_text Q15 Africa continent on the Earth’s northern and southern hemispheres fr Afrique Q181238 Africa Roman province on the northern African coast covering parts of present-day Tunisia, Algeria, and Libya fr Afrique Q203548 African Plate continental plate underlying Africa fr Afrique Q258 South Africa country in Southern Africa fr Afrique du Sud Q4412 West Africa westernmost region of the African continent fr Afrique de l’Ouest Q132959 Sub-Saharan Africa area of the continent of Africa that lies south of the Sahara Desert fr Afrique subsaharienne Q27433 Central Africa core region of the African continent fr Afrique centrale Q27394 Southern Africa southernmost region of the African continent fr Afrique australe Q27407 East Africa easterly region of the African continent fr Afrique de l’Est Q27381 North Africa northernmost region of the African continent fr Afrique du Nord Q23639892 Africa artwork by Eugène Delaplanche in Paris, France fr Afrique Q56317928 Afrique ship fr Afrique Q66022909 Afrique NA fr Afrique Q153963 German East Africa former German posesssion in the African Great Lakes region between 1884–1919 fr Afrique orientale allemande Q4690138 Afrique album by Count Basie fr Afrique Q65574303 Afrique NA fr Afrique Q210682 French West Africa French colonial federation (1895–1958) fr Afrique-Occidentale française Q106179043 Afrique NA en Afrique Q66065 Sahel ecoclimatic and biogeographic transition zone in Africa fr Afrique sub-sahélienne Q271894 French Equatorial Africa federation of French colonial possessions in Central Africa fr Afrique-Équatoriale française As we can see, many of the entities proposed in he list are not interesting and we will probably have to select one by one the entities of interest. But we have clearly to keep two different list of entities : the target entities : that we consider as potential world regions or candidate to te title of “geographic imagination”. the control entites : that we have to identify or eliminate if we want to identify correctly our target entities like the country of South Africa In the case of Africa, we could for example establish a more limited list entit &lt;- c(&quot;Q15&quot;, &quot;Q4412&quot;,&quot;Q132959&quot;, &quot;Q27394&quot;,&quot;Q27407&quot;,&quot;Q27381&quot;,&quot;Q27433&quot;,&quot;Q258&quot;) tab&lt;-tab %&gt;% filter(item_id %in% entit) kable(tab) item_id item_label item_desc item_lang item_text Q15 Africa continent on the Earth’s northern and southern hemispheres fr Afrique Q258 South Africa country in Southern Africa fr Afrique du Sud Q4412 West Africa westernmost region of the African continent fr Afrique de l’Ouest Q132959 Sub-Saharan Africa area of the continent of Africa that lies south of the Sahara Desert fr Afrique subsaharienne Q27433 Central Africa core region of the African continent fr Afrique centrale Q27394 Southern Africa southernmost region of the African continent fr Afrique australe Q27407 East Africa easterly region of the African continent fr Afrique de l’Est Q27381 North Africa northernmost region of the African continent fr Afrique du Nord But this list which was based on the french textual units associated to “Afrique” should certainly be completed by equivalent list established for other languages with different seeds (“Africa” in english, “Afrika” in german, …) 4.3.2 Elaboration of a cross_linguistic dictionnary Admitting that we have established a list of wikipedia entities of interest, we can now turn to the creation of a dictionary for the identification of these entities in different languages. We will use for that purpose the powerful function get_properties item_prop &lt;- get_property(&quot;Q15&quot;)[[1]] The result is a very large object (list of list) which provide all the informations (or links toward these information) in all languages wher the object is available. The problem is therefore to understand the structure of this object and to extract exactly what we need. In our case, we want to extract for each language of interest. The information will be separated in two datasets : dictionary of definitions dictionary of labels and aliases We create two functions dedicated to each of the tasks extract_def &lt;- function(item = c(&quot;Q15&quot;, &quot;Q246&quot;), langs = c(&quot;fr&quot;,&quot;de&quot;,&quot;en&quot;,&quot;tr&quot;,&quot;ar&quot;)) { # Create empty dataset res&lt;-data.frame() res$id &lt;- as.character() res$lang &lt;- as.character() res$label &lt;- as.character() res$desc &lt;- as.character() # Loop of items n &lt;- length(item) for (i in 1:n) { # Extract item properties item_prop &lt;- get_property(item[i])[[1]] # Loop of language p&lt;-length(langs) for (j in 1:p) { id &lt;- item[i] lang &lt;- langs[j] if(is.null(item_prop[[&quot;labels&quot;]][[lang]]$value)==F) {label &lt;- item_prop[[&quot;labels&quot;]][[lang]]$value} else { label &lt;- NA} if(is.null(item_prop[[&quot;descriptions&quot;]][[lang]]$value)==F) {desc &lt;- item_prop[[&quot;descriptions&quot;]][[lang]]$value} else { desc &lt;- NA} add &lt;-data.frame(id,lang,label,desc) res&lt;- rbind(res,add) } } # Export result return(res) } The function works proprerly as long as the entities are available in all languages. It should be adapted to prevent errors when an entity is not available in one language. entit &lt;- c(&quot;Q15&quot;, &quot;Q4412&quot;,&quot;Q132959&quot;, &quot;Q27394&quot;,&quot;Q27407&quot;,&quot;Q27381&quot;,&quot;Q27433&quot;,&quot;Q258&quot;) tab&lt;-extract_def(entit,c(&quot;fr&quot;,&quot;de&quot;,&quot;tr&quot;,&quot;en&quot;,&quot;ar&quot;)) kable(tab) id lang label desc Q15 fr Afrique continent Q15 de Afrika Kontinent auf der Nord- und Südhalbkugel der Erde Q15 tr Afrika Dünya’nın kuzey ve güney yarıkürelerindeki bir kıta Q15 en Africa continent on the Earth’s northern and southern hemispheres Q15 ar إفريقيا ثاني أكبر قارات العالم من حيث المساحة وعدد السكان، تأتي في المرتبة الثانية بعد آسيا Q4412 fr Afrique de l’Ouest région d’Afrique Q4412 de Westafrika Kontinentalteil Q4412 tr Batı Afrika Afrika’nın batısındaki 16 ülkenin bulunduğu alan Q4412 en West Africa westernmost region of the African continent Q4412 ar غرب أفريقيا المنطقة الغربية للقارة الأفريقية Q132959 fr Afrique subsaharienne partie du continent africain au sud du Sahara Q132959 de Subsahara-Afrika südlich der Sahara gelegener Teil Afrikas Q132959 tr Sahraaltı Afrika NA Q132959 en Sub-Saharan Africa area of the continent of Africa that lies south of the Sahara Desert Q132959 ar أفريقيا جنوب الصحراء جزء من القارة الأفريقية يقع جنوب الصحراء الكبرى Q27394 fr Afrique australe région la plus méridionale du continent africain Q27394 de Südliches Afrika Region in Afrika Q27394 tr Güney Afrika NA Q27394 en Southern Africa southernmost region of the African continent Q27394 ar أفريقيا الجنوبية المنطقة الجنوبية للقارة الأفريقية Q27407 fr Afrique de l’Est région d’Afrique Q27407 de Ostafrika Region in Afrika Q27407 tr Doğu Afrika NA Q27407 en East Africa easterly region of the African continent Q27407 ar شرق أفريقيا المنطقة الشرقية للقارة الأفريقية Q27381 fr Afrique du Nord région en Afrique Q27381 de Nordafrika Region in Afrika Q27381 tr Kuzey Afrika Afrika kıtasının Fas, Cezayir, Tunus, Libya, Mısır ve Sudan’ı içeren kuzey bölgesi Q27381 en North Africa northernmost region of the African continent Q27381 ar شمال أفريقيا المناطق الجغرافية التي تقع شمال أفريقيا بشكل عام Q27433 fr Afrique centrale Région d’Afrique Q27433 de Zentralafrika Region in Afrika Q27433 tr Orta Afrika Afrika kıtasının Burundi, Orta Afrika Cumhuriyeti, Çad, Kongo Demokratik Cumhuriyeti ve Ruanda’yı barındıran orta kısmı Q27433 en Central Africa core region of the African continent Q27433 ar وسط أفريقيا منطقة رئيسية للقارة الأفريقية Q258 fr Afrique du Sud pays d’Afrique Q258 de Südafrika Staat im Süden Afrikas Q258 tr Güney Afrika Cumhuriyeti Güney Afrika’da bulunan bir ülke Q258 en South Africa country in Southern Africa Q258 ar جنوب أفريقيا دولة في أفريقيا الجنوبية 4.3.3 Extraction of aliases Now we have to extract the aliases which are two texts corresponding to the same entity in a given,language. For example, the Q27394 which correspond to the southern part of Africa (a subregion, not a country) is associated in spanish language to one main label and three equivalenbt alisases item_prop &lt;- get_property(&quot;Q27394&quot;)[[1]] item_prop$labels$es$value [1] &quot;África austral&quot; item_prop$aliases$es language value 1 es África del Sur 2 es sur de África 3 es África de l sureste But in french language, no aliases are mentioned : item_prop$labels$fr$value [1] &quot;Afrique australe&quot; item_prop$aliases$fr NULL The fact that no aliases are mentioned in french language can be considered as non logical as compared to spanish language. And we could certainly imagine to add in french the translation of two spanish aliases: “Afrique méridionale”, “Sud de l’Afrique”. But we can not add “Afrique du Sud” because it is related in french to the state and not to the subregion. Despite the fact that they are not complete, the aliases are certainly a good solution when we want to obtain more efficient dictionaries. For example, if we want to obtain the state of southern Africa (Q258), we can complete the official label by 4 alias in french language and 3 aliases in spanish, taking into account the fact that the text is in upper orlowercase, withor without accent, … item_prop &lt;- get_property(&quot;Q258&quot;)[[1]] item_prop$labels$es$value [1] &quot;Sudáfrica&quot; item_prop$aliases$es language value 1 es República de Sudáfrica 2 es Sudafrica 3 es Republica de Sudafrica item_prop$labels$fr$value [1] &quot;Afrique du Sud&quot; item_prop$aliases$fr language value 1 fr République sud-africaine 2 fr République d’Afrique du Sud 3 fr république sud-africaine 4 fr république d’Afrique du Sud lang=&quot;fr&quot; is.null(item_prop[[&quot;aliases&quot;]][[lang]])!=F [1] FALSE ali &lt;- item_prop[[&quot;aliases&quot;]][[lang]]$value n&lt;-length(ali) for (i in 1:n) { print(ali[i])} [1] &quot;République sud-africaine&quot; [1] &quot;République d’Afrique du Sud&quot; [1] &quot;république sud-africaine&quot; [1] &quot;république d’Afrique du Sud&quot; We propose therefore a function called extract_alias which propose for each entity of interest a list of texts and aliases adapte to each language. We do not store the definition which has been otained previously with the function extract_def : extract_alias &lt;- function(items = c(&quot;Q15&quot;, &quot;Q258&quot;), langs = c(&quot;fr&quot;,&quot;de&quot;,&quot;en&quot;,&quot;tr&quot;,&quot;ar&quot;)) { # Create empty dataset res&lt;-data.frame() res$id &lt;- as.character() res$lang &lt;- as.character() res$label &lt;- as.character() # Loop of items n &lt;- length(items) for (i in 1:n) { # Extract item properties item_prop &lt;- get_property(items[i])[[1]] # Loop of language p&lt;-length(langs) for (j in 1:p) { id &lt;- items[i] lang &lt;- langs[j] if(is.null(item_prop[[&quot;labels&quot;]][[lang]]$value)==F) {label &lt;- item_prop[[&quot;labels&quot;]][[lang]]$value} else { label &lt;- NA} if(is.null(item_prop[[&quot;descriptions&quot;]][[lang]]$value)==F) {desc &lt;- item_prop[[&quot;descriptions&quot;]][[lang]]$value}else { desc &lt;- NA} add &lt;-data.frame(id,lang,label) res&lt;- rbind(res,add) # Loop of aliases if (is.null(item_prop[[&quot;aliases&quot;]][[lang]])==F) { ali &lt;- item_prop[[&quot;aliases&quot;]][[lang]]$value n&lt;-length(ali) for (k in 1:n) { label &lt;- ali[k] add &lt;-data.frame(id,lang,label) res&lt;- rbind(res,add) } } } } # Export result return(res) } Let’s try the function on the case of the continent of “Africa” (Q15), the subregion “South of Africa” (Q27394) and the state of “Southern Africa” (Q258) in five languages : tab&lt;- extract_alias(items = c(&quot;Q15&quot;, &quot;Q27394&quot;, &quot;Q258&quot;), langs = c(&quot;fr&quot;,&quot;de&quot;,&quot;en&quot;,&quot;tr&quot;,&quot;ar&quot;)) kable(tab) id lang label Q15 fr Afrique Q15 de Afrika Q15 en Africa Q15 en African continent Q15 en Ancient Libya Q15 tr Afrika Q15 tr Afrika kıtası Q15 ar إفريقيا Q15 ar القارة الأفريقية Q15 ar أفريقيا Q15 ar إفريقية Q15 ar أفريقية Q27394 fr Afrique australe Q27394 de Südliches Afrika Q27394 de Südafrika Q27394 en Southern Africa Q27394 tr Güney Afrika Q27394 ar أفريقيا الجنوبية Q27394 ar إفريقيا الجنوبية Q27394 ar جنوبي أفريقيا Q27394 ar أفريقيا الجنوبيه Q27394 ar جنوب أفريقيا (منطقة) Q258 fr Afrique du Sud Q258 fr République sud-africaine Q258 fr République d’Afrique du Sud Q258 fr république sud-africaine Q258 fr république d’Afrique du Sud Q258 de Südafrika Q258 de Suedafrika Q258 de Republik Südafrika Q258 en South Africa Q258 en Republic of South Africa Q258 en RSA Q258 en SA Q258 en za Q258 en 🇿🇦 Q258 en zaf Q258 tr Güney Afrika Cumhuriyeti Q258 ar جنوب أفريقيا The function works ! 4.3.4 Conclusion It is now possible to develop a global research strategy for the analysis of world regions : 1. Define a set of target regions in one language : In our example, it was based on the use of the term “Afrique” in french language, but we can imagine a different list. 2. Identify the code of the wikidata entities associated to this target regions : We have generally a lot of entities of minor interest. 3. Identify the code of the other wikidata entities that should be added for control : As we have seen, some entities are likely to create confusion and ambiguity in the definition of target entities. This entity will be transformed in compound or eliminate from the text before to look for the target entities. 4. Extract the properties of the entities in the different languages of interest : this step can be an opportunity to return to step 1. For example, it it appears that some subdivisions of Africa are available in english or german language but not in french. 5. Compare the definitions of Wikipedia entities in different languages : it is important to check if the assumption of identity of the entities is correct or not. If not, some entities will be eliminated from the list. 6. Extract the dictionary of recognition of entities : which can be done in a multilanguage perspective. It is obviously possible to apply the same procedure to different objects like states, capital cities, organizations, people, etc… second largest continent in the world in terms of area and population, comes second only to Asia.↩︎ "],["22-Dicogeo.html", "Chapter 5 Dictionary of geographical entities 5.1 Dictionary of geographical entities", " Chapter 5 Dictionary of geographical entities library(knitr) library(dplyr) library(WikidataR) library(quanteda) 5.1 Dictionary of geographical entities We realize a test of the method described above on a preliminary dictionary of world regions combined with world states described by their names and capital cities 5.1.1 Load the list of world regions, organisation states and capital cities We start from a provisional list of 65 world regions based on continents (cont), other land area (14), sea regions (sea), regional organisations (org), states names (sta) and capital cities of states (cap). ent&lt;-read.table(&quot;dict/worldgeo_codes.csv&quot;, sep=&quot;;&quot;, header=T, encoding = &quot;UTF-8&quot;) table(ent$type) cap cont land org sea sta 194 36 14 44 16 195 5.1.2 Extract définitions We extract the definitions of the regions in the different languages with the function extract_def() ## NOT RUN : need several minutes !!!## worldgeo_def &lt;- extract_def(ent$wikidata,c(&quot;fr&quot;,&quot;de&quot;,&quot;en&quot;,&quot;tr&quot;,&quot;ar&quot;)) names(ent)&lt;-c(&quot;id&quot;,&quot;type_ent&quot;) worldgeo_def&lt;-worldreg_def %&gt;% left_join(ent) write.table(x = worldgeo_def, row.names = FALSE, file = &quot;dict/worldgeo_def.csv&quot;, fileEncoding = &quot;UTF-8&quot;, sep = &quot;;&quot;) saveRDS(object = worldgeo_def, file = &quot;dict/worldgeo_def.RDS&quot;) The dictionary of entities is further analyzed manually and completed by an internal code of entities specific to IMAGEUN’s project 5.1.3 Extract aliases and create dictionary ## NOT RUN : very long time ...### worldgeo_def &lt;- readRDS(&quot;dict/worldgeo_def.RDS&quot;) worldgeo_dict &lt;- extract_alias(worldgeo_def$id, c(&quot;fr&quot;,&quot;de&quot;,&quot;en&quot;,&quot;tr&quot;,&quot;ar&quot;)) write.table(x = worldgeo_dict, row.names = FALSE, file = &quot;dict/worldgeo_dict.csv&quot;, fileEncoding = &quot;UTF-8&quot;, sep = &quot;;&quot;) saveRDS(object = worldgeo_dict, file = &quot;dict/worldgeo_dict.RDS&quot;) "],["23-Taggeo.html", "Chapter 6 Geographical tags 6.1 Manual correctio n of dictionnary 6.2 Detection of geographical entities", " Chapter 6 Geographical tags library(knitr) library(dplyr) library(WikidataR) library(quanteda) 6.1 Manual correctio n of dictionnary The dictionnary elaboated by automatic procedures has been manually corrected and the new version is upload. dict&lt;-read.table(&quot;dict/worldgeo_dict_V4bis.csv&quot;, header=T, sep=&quot;;&quot;, encoding = &quot;UTF-8&quot;, quote = &#39;&quot;&#39;) 6.2 Detection of geographical entities We have elaborated a function for the extraction of geographical units based on the dictionary elaborated in previous section (dict) according to the language (lang), the decision to split some tokens (split) to move or not to lower case (tolow) and the possibility to add a list of compounds to be realized (comps) in order to eliminate ambiguities. extract_tags &lt;- function(qd = qd, # the corpus of interest lang = &quot;fr&quot;, # the language to be used dict = dict, # the dictionary of target code = &quot;id&quot; , # variable used for coding split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), # split list tolow = FALSE , # Tokenize text comps = c(&quot;Afrique du sud&quot;) # compounds ) { # Tokenize x&lt;-as.character(qd) if(length(split) &gt; 0) { reg&lt;-paste(split, collapse = &#39;|&#39;) x &lt;- gsub(reg,&quot; &quot;,x)} if(tolow) { x &lt;- tolower(x)} toks&lt;-tokens(x) # compounds if(length(split) &gt; 0) { reg&lt;-paste(split, collapse = &#39;|&#39;) comps&lt;- gsub(reg,&quot; &quot;,comps)} if(tolow) {comps &lt;- tolower(comps)} toks&lt;-tokens_compound(toks,pattern=phrase(comps)) # Load dictionaries and create compounds ## Target dictionary dict&lt;-dict[dict$lang==lang &amp; is.na(dict$label)==F,] target&lt;-dict[ntoken(dict$label)&gt;1,] labels &lt;-dict$label if(length(split) &gt; 0) { reg&lt;-paste(split, collapse = &#39;|&#39;) labels&lt;- gsub(reg,&quot; &quot;,labels)} if(tolow) {labels &lt;- tolower(labels)} toks&lt;-tokens_compound(toks,pattern=phrase(labels)) # create quanteda dictionary keys &lt;-gsub(&quot; &quot;,&quot;_&quot;,labels) qd_dict&lt;-as.list(keys) names(qd_dict)&lt;-dict[[code]] qd_dict&lt;-dictionary(qd_dict,tolower = FALSE) # Identify geo tags (states or reg or org ...) toks_tags &lt;- tokens_lookup(toks, qd_dict, case_insensitive = F) toks_tags &lt;- lapply(toks_tags, unique) toks_tags&lt;-as.tokens(toks_tags) list_tags&lt;-function(x){res&lt;-paste(x, collapse=&#39; &#39;)} docvars(qd)[[&quot;tags&quot;]]&lt;-as.character(lapply(toks_tags,FUN=list_tags)) docvars(qd)[[&quot;nbtags&quot;]]&lt;-ntoken(toks_tags) # Export results return(qd) } 6.2.1 Le Figaro (FRA) #dict&lt;-readRDS(&quot;dict/worldgeo_dict_V4.RDS&quot;) qd &lt;- readRDS(&quot;quanteda/fr_FRA_figaro.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) frcomps&lt;-c(&quot;Europe 1&quot;, &quot;Atlantic city&quot;, &quot;Loire-Atlantique&quot;, &quot;Pyrénées-Atlantique&quot;, &quot;Pyrénées-Atlantiques&quot;, &quot;Alpes-de-Haute-Provence&quot;, &quot;Hautes-Alpes&quot;, &quot;Rhöne-Alpes&quot;,&quot;Alpes-Maritimes&quot;, &quot;Chantiers de l&#39;Atlantique&quot;, &quot;TGV Atlantique&quot;, &quot;Bourse de Paris&quot;, &quot;Paris SG&quot;, &quot;Ville de Paris&quot;, &quot;Grand Paris&quot;) qd &lt;- extract_tags (qd = qd, lang=&quot;fr&quot;, dict = dict, code = &quot;code&quot;, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), comps = frcomps, tolow = FALSE) saveRDS(qd,&quot;quanteda/fr_FRA_figaro_geo.RDS&quot;) qd1&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),regs=qd$tags,nbregs=qd$nbtags) 6.2.2 Le Monde qd &lt;- readRDS(&quot;quanteda/fr_FRA_lmonde.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;fr&quot;, dict = dict, code = &quot;code&quot;, comps = frcomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/fr_FRA_lmonde_geo.RDS&quot;) qd2&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.3 FAZ qd &lt;- readRDS(&quot;quanteda/de_DEU_frankf.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) decomps &lt;- c(&quot;Europa League&quot;) qd &lt;- extract_tags (qd = qd, lang=&quot;de&quot;, dict = dict, code = &quot;code&quot;, comps = decomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/de_DEU_frankf_geo.RDS&quot;) qd3&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.4 Süddeutsche Zeitung qd &lt;- readRDS(&quot;quanteda/de_DEU_suddeu.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;de&quot;, dict = dict, code = &quot;code&quot;, comps = decomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/de_DEU_suddeu_geo.RDS&quot;) qd4&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.5 Guardian qd &lt;- readRDS(&quot;quanteda/en_GBR_guardi.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) encomps&lt;-c(&quot;Atlantic City&quot;, &quot;Cathay Pacific&quot;, &quot;Virgin Atlantic&quot;) qd &lt;- extract_tags (qd = qd, lang=&quot;en&quot;, dict = dict, code = &quot;code&quot;, comps = encomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/en_GBR_guardi_geo.RDS&quot;) qd5&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.6 Daily Telegraph qd &lt;- readRDS(&quot;quanteda/en_GBR_telegr.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;en&quot;, dict = dict, code = &quot;code&quot;, comps = encomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/en_GBR_telegr_geo.RDS&quot;) qd6&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.7 Belfast Telegraph qd &lt;- readRDS(&quot;quanteda/en_NIR_beltel.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;en&quot;, dict = dict, code = &quot;code&quot;, comps = encomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/en_NIR_beltel_geo.RDS&quot;) qd7&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.8 Irish Times qd &lt;- readRDS(&quot;quanteda/en_IRL_irtime.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;en&quot;, dict = dict, code = &quot;code&quot;, comps = encomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/en_IRL_irtime_geo.RDS&quot;) qd8&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.9 Cumhuryet qd &lt;- readRDS(&quot;quanteda/tr_TUR_cumhur.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) trcomps &lt;- c(&quot;Cathay Pacific&quot;, &quot;Avrupa Ligi&quot;) qd &lt;- extract_tags (qd = qd, lang=&quot;tr&quot;, dict = dict, code = &quot;code&quot;, comps = trcomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/tr_TUR_cumhur_geo.RDS&quot;) qd9&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.10 Yeni Safak qd &lt;- readRDS(&quot;quanteda/tr_TUR_yenisa.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;tr&quot;, dict = dict, code = &quot;code&quot;, comps = trcomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/tr_TUR_yenisa_geo.RDS&quot;) qd10&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.11 Al Nahar (DZA) arcomps = c(&quot;الصحراء الغربية&quot;) qd &lt;- readRDS(&quot;quanteda/ar_DZA_alnaha.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;ar&quot;, dict = dict, code = &quot;code&quot;, comps = arcomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/ar_DZA_alnaha_geo.RDS&quot;) qd11&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.12 El Khabar (DZA) qd &lt;- readRDS(&quot;quanteda/ar_DZA_elkahb.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;ar&quot;, dict = dict, code = &quot;code&quot;, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/ar_DZA_elkahb_geo.RDS&quot;) qd12&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.13 Global corpus qd &lt;- c(qd1,qd2,qd3,qd4,qd5,qd6,qd7,qd8,qd9,qd10,qd11,qd12) saveRDS(qd,&quot;quanteda/corpus_worldgeo_V2.RDS&quot;) table(qd$nbtags) "],["24-Corpus.html", "Chapter 7 Corpus 7.1 Objectives 7.2 Prepare data 7.3 Distribution des nouvelles par média au cours du temps 7.4 Fréquences des macro-régions par journal 7.5 Fréquence des macro-régions par mois 7.6 Distribution des nouvelles sur les macro-régions par média au cours du temps", " Chapter 7 Corpus library(knitr) library(dplyr) library(quanteda) library(data.table) library(tidytext) library(ggplot2) 7.1 Objectives The aim of this section is to separate states and world region and to evaluate the relative frequencies of world regions by media an through time. 7.2 Prepare data qd&lt;-readRDS(&quot;quanteda/corpus_worldgeo_V2.RDS&quot;) qd$regs&lt;-qd$tags qd$regs&lt;-gsub(&quot;ST_...&quot;,&quot;&quot;,qd$tags) qd$regs&lt;-gsub(&quot;CA_...&quot;,&quot;&quot;,qd$regs) qd$nbregs&lt;-ntoken(tokens(as.character(qd$regs))) td&lt;-tidy(qd) hypercube &lt;-function(qd = qd, when = &quot;date&quot;, when_cut = &quot;year&quot;, who = &quot;source&quot;, where1 = &quot;tags&quot;, where2 = &quot;tags&quot;) { # create data.table accroding to parameter chosen don&lt;-docvars(qd) df&lt;-data.table(id = docid(qd), who = don[,who], when = as.character(cut(don[,when],breaks=when_cut)), where1 = don[,where1], where2 = don[,where2]) # add code _no_ for empty fields df$where1[df$where1==&quot;&quot;]&lt;-&quot;_no_&quot; df$where2[df$where2==&quot;&quot;]&lt;-&quot;_no_&quot; # unnest where1 df&lt;-unnest_tokens(df,where1,where1,to_lower=F) # unnest where2 df&lt;-unnest_tokens(df,where2,where2,to_lower=F) # define number of occurence by id nb&lt;-df[,.N,list(id)] %&gt;% mutate(wgt = 1/N) %&gt;% select(-N) df&lt;-df %&gt;% left_join(nb) rm(nb) # Aggregate hc&lt;- df[,.( tags = .N, news=sum(wgt)) ,.(who, when,where1,where2)] # Convert date to time hc$when&lt;-as.Date(hc$when) # return hypercube return(hc) } hc_reg &lt;- hypercube(qd = qd, when = &quot;date&quot;, when_cut = &quot;months&quot;, who = &quot;source&quot;, where1 = &quot;regs&quot;, where2 = &quot;regs&quot;) saveRDS(hc_reg,&quot;hypercube/hc_reg.RDS&quot;) 7.3 Distribution des nouvelles par média au cours du temps hc_reg&lt;-readRDS(&quot;hypercube/hc_reg.RDS&quot;) hc_reg$OK&lt;-as.factor(hc_reg$where1!=&quot;_no_&quot;) levels(hc_reg$OK)&lt;-c(&quot;Non&quot;,&quot;Oui&quot;) month&lt;-hc_reg[,.(nb&lt;-sum(news)),.(when,who,OK)] %&gt;% dcast(formula = who+when~OK, value.var = &quot;V1&quot;,fill = 0) %&gt;% mutate(Media=who, Date = when, Total=Non+Oui, Frequence = Oui, Pourcentage = 100*Frequence/Total) %&gt;% select(Media,Date, Total,Frequence, Pourcentage) %&gt;% filter(is.na(Date)==F, Date &lt; as.Date(&quot;2021-07-01&quot;)) #kable(month,digits=c(0,0,0,2),caption = &quot;Parts des nouvelles mentionnant une macro-région&quot;) ggplot(month,aes(x=Date,y=Total,color=Media)) + geom_line(lwd=0.5) + ggtitle(&quot;Nombre de nouvelles par média (Jan. 2019- Juin 2021)&quot;, subtitle = &quot;source : Mediacloud)&quot; ) 7.4 Fréquences des macro-régions par journal hc_reg&lt;-readRDS(&quot;hypercube/hc_reg.RDS&quot;) hc_reg$OK&lt;-as.factor(hc_reg$where1!=&quot;_no_&quot;) levels(hc_reg$OK)&lt;-c(&quot;Non&quot;,&quot;Oui&quot;) med&lt;-hc_reg[,.(nb&lt;-sum(news)),.(who,OK)] %&gt;% dcast(formula = who~OK, value.var = &quot;V1&quot;) %&gt;% mutate(Media = who, Total=Non+Oui, Frequence = Oui, Pourcentage = 100*Frequence/Total) %&gt;% select(Media, Total,Frequence, Pourcentage) tot&lt;-med[1,] tot$Media&lt;-&quot;Total&quot; tot$Total&lt;-sum(med$Total) tot$Frequence&lt;-sum(med$Frequence) tot$Pourcentage&lt;-100*tot$Frequence/tot$Total tabres&lt;-rbind(med,tot) kable(tabres,digits=c(0,0,0,2),caption = &quot;Parts des nouvelles mentionnant une macro-région&quot;) Table 7.1: Parts des nouvelles mentionnant une macro-région Media Total Frequence Pourcentage ar_DZA_alnaha 32690 525 1.61 ar_DZA_elkahb 53401 509 0.95 de_DEU_frankf 86802 2203 2.54 de_DEU_suddeu 34858 1186 3.40 en_GBR_guardi 75064 1711 2.28 en_GBR_telegr 37122 967 2.60 en_IRL_irtime 104956 2226 2.12 en_NIR_beltel 97861 1172 1.20 fr_FRA_figaro 124092 3406 2.74 fr_FRA_lmonde 47888 1791 3.74 tr_TUR_cumhur 128715 2593 2.01 tr_TUR_yenisa 92575 2630 2.84 Total 916024 20919 2.28 -Commentaire : Sur un total de 916024 titres de nouvelles, 20619 contenaient au moins une macro-région, ce qui représente une proportion de 2.28% soit environ une nouvelle sur cinquante. Cette proportion varoe de 1 à 4% selon les journaux. 7.5 Fréquence des macro-régions par mois hc_reg&lt;-readRDS(&quot;hypercube/hc_reg.RDS&quot;) hc_reg$OK&lt;-as.factor(hc_reg$where1!=&quot;_no_&quot;) levels(hc_reg$OK)&lt;-c(&quot;Non&quot;,&quot;Oui&quot;) month&lt;-hc_reg[,.(nb&lt;-sum(news)),.(when,OK)] %&gt;% dcast(formula = when~OK, value.var = &quot;V1&quot;) %&gt;% mutate(Date = when, Total=Non+Oui, Frequence = Oui, Pourcentage = 100*Frequence/Total) %&gt;% select(Date, Total,Frequence, Pourcentage) %&gt;% filter(is.na(Date)==F, Date &lt; as.Date(&quot;2021-07-01&quot;)) kable(month,digits=c(0,0,0,2),caption = &quot;Parts des nouvelles mentionnant une macro-région&quot;) Table 7.2: Parts des nouvelles mentionnant une macro-région Date Total Frequence Pourcentage 2019-01-01 22301 440 1.97 2019-02-01 19120 382 2.00 2019-03-01 19572 490 2.50 2019-04-01 19164 448 2.34 2019-05-01 20404 525 2.57 2019-06-01 18726 467 2.49 2019-07-01 18823 498 2.65 2019-08-01 17471 332 1.90 2019-09-01 16138 281 1.74 2019-10-01 16076 390 2.43 2019-11-01 14848 304 2.05 2019-12-01 15263 326 2.14 2020-01-01 16684 391 2.34 2020-02-01 18764 426 2.27 2020-03-01 25632 573 2.24 2020-04-01 25703 544 2.12 2020-05-01 26096 553 2.12 2020-06-01 26849 568 2.12 2020-07-01 26307 643 2.44 2020-08-01 25513 569 2.23 2020-09-01 37023 1012 2.73 2020-10-01 40691 887 2.18 2020-11-01 41480 841 2.03 2020-12-01 40239 1125 2.80 2021-01-01 40145 931 2.32 2021-02-01 37853 834 2.20 2021-03-01 42793 1050 2.45 2021-04-01 32958 780 2.37 2021-05-01 40671 991 2.44 2021-06-01 40781 1107 2.71 ggplot(month,aes(x=Date,y=Pourcentage)) + geom_line(col=&quot;red&quot;,lwd=0.5) + geom_point(col=&quot;black&quot;) + geom_smooth() + ggtitle(&quot;Les macro-régions dans les nouvelles de presse quotienne (Jan. 2019- Juin 2021)&quot;, subtitle = &quot;12 médias de France, Allemagne, Turquie, Tunisie, Irlande et Royaume-Uni (source : Mediacloud)&quot; ) 7.6 Distribution des nouvelles sur les macro-régions par média au cours du temps hc_reg&lt;-readRDS(&quot;hypercube/hc_reg.RDS&quot;) hc_reg$OK&lt;-as.factor(hc_reg$where1!=&quot;_no_&quot;) levels(hc_reg$OK)&lt;-c(&quot;Non&quot;,&quot;Oui&quot;) month&lt;-hc_reg[,.(nb&lt;-sum(news)),.(when,who,OK)] %&gt;% dcast(formula = who+when~OK, value.var = &quot;V1&quot;,fill = 0) %&gt;% mutate(Media=who, Date = when, Total=Non+Oui, Frequence = Oui, Pourcentage = 100*Frequence/Total) %&gt;% select(Media,Date, Total,Frequence, Pourcentage) %&gt;% filter(is.na(Date)==F, Date &lt; as.Date(&quot;2021-07-01&quot;)) #kable(month,digits=c(0,0,0,2),caption = &quot;Parts des nouvelles mentionnant une macro-région&quot;) ggplot(month,aes(x=Date,y=Pourcentage,color=Media)) + geom_smooth(lwd=0.6,fill=NA) + ggtitle(&quot;Part des nouvelles sur les macro-régions par média (Jan. 2019- Juin 2021)&quot;, subtitle = &quot;source : Mediacloud&quot; ) "],["31-TopRegions.html", "Chapter 8 Top World regions 8.1 Data 8.2 Top 50 regions in full corpus 8.3 Top 10 regions by media 8.4 Synthesis by correspondance analysis and Hierarchical Clustering 8.5 Synthesis by chi-square &amp; heatmap", " Chapter 8 Top World regions 8.1 Data We laod an hypercube where the text of news has been removed and where we keep only the number of tags or proportion of news speaking from one or several regions (where1, where2), by media (who) and by time period (when) # Load Hypercube and select period hc_reg&lt;-readRDS(&quot;hypercube/hc_reg.RDS&quot;) hc_reg %&gt;% filter(is.na(when)==F, when &lt; as.Date(&quot;2021-01-01&quot;)) who when where1 where2 tags news 1: fr_FRA_figaro 2019-01-01 _no_ _no_ 3262 3262.00 2: fr_FRA_figaro 2019-01-01 CO_EUR CO_EUR 15 15.00 3: fr_FRA_figaro 2019-01-01 SE_medit SE_medit 2 2.00 4: fr_FRA_figaro 2019-01-01 OR_AfrUn OR_AfrUn 2 2.00 5: fr_FRA_figaro 2019-01-01 OR_EU OR_EU 19 19.00 --- 3653: ar_DZA_elkahb 2020-12-01 CO_EUR CO_EUR 3 3.00 3654: ar_DZA_elkahb 2020-12-01 LA_maghr LA_maghr 1 1.00 3655: ar_DZA_elkahb 2020-12-01 LA_east_middle LA_east_middle 1 0.25 3656: ar_DZA_elkahb 2020-12-01 LA_east_middle CO_AFR 1 0.25 3657: ar_DZA_elkahb 2020-12-01 CO_AFR LA_east_middle 1 0.25 # Define weights correction for equal contribution of media wgt&lt;-hc_reg[,list(coeff = sum(news)), list(who)] %&gt;% mutate(coeff=mean(coeff)/coeff) hc_reg&lt;-left_join(hc_reg,wgt) Joining, by = &quot;who&quot; hc_reg$news_wgt&lt;-hc_reg$news*hc_reg$coeff # Load table of label and choose language reg_def&lt;-readRDS(&quot;dict/worldgeo_def_V1.RDS&quot;) tab_def&lt;-reg_def %&gt;% filter(lang==&quot;fr&quot;) %&gt;% select(code,type,label) tab_def$label[tab_def$code==&quot;OR_NATO&quot;]&lt;-&quot;OTAN&quot; 8.2 Top 50 regions in full corpus 8.2.1 Unweighted We can propose firstly a table of top entities in the whole corpus of newspapers with index 100 for the first entity. # Compute df&lt;-hc_reg[where1 !=&quot;_no_&quot;,list(nb = sum(news)), list(where1)] df&lt;-merge(tab_def,df,by.x=&quot;code&quot;,by.y=&quot;where1&quot;,all.x=F,all.y=T) df&lt;-df[order(df$nb, decreasing = T),] row.names(df)&lt;-1:dim(df)[1] df$index&lt;-100*df$nb/max(df$nb) kable(head(df,50), digits=c(NA,NA,NA,0,2),row.names = T) code type label nb index 1 OR_EU org Union européenne 7621 100.00 2 CO_EUR cont Europe 5347 70.17 3 CO_AFR cont Afrique 1373 18.01 4 SE_medit sea mer Méditerranée 960 12.60 5 OR_NATO org OTAN 829 10.88 6 CO_ASI_minor cont Asie mineure 499 6.55 7 SE_black sea mer Noire 477 6.26 8 CO_ASI cont Asie 350 4.59 9 LA_east_middle land Moyen-Orient 348 4.57 10 LA_alpen land Alpes 253 3.32 11 CO_AMR cont Amérique 230 3.01 12 LA_sahel land Sahel 209 2.74 13 SE_arcti sea Arctique 201 2.64 14 CO_AMR_latin cont Amérique latine 148 1.95 15 LA_amazon land Amazonie 139 1.83 16 CO_AFR_south cont Afrique australe 130 1.70 17 CO_ANT cont Antarctique 116 1.52 18 OR_CoEur org Conseil de l’Europe 94 1.24 19 SE_carai sea Caraïbes 93 1.22 20 LA_sahara land Sahara 84 1.10 21 SE_polyn sea Polynésie 83 1.09 22 LA_east_near land Proche-Orient 82 1.08 23 LA_maghr land Machrek 80 1.05 24 CO_AMR_south cont Amérique du Sud 78 1.03 25 CO_ERA cont Eurasie 70 0.92 26 LA_balka land Balkans 59 0.77 27 CO_AFR_west cont Afrique de l’Ouest 58 0.75 28 CO_AMR_centr cont Amérique centrale 54 0.71 29 OR_Merco org marché commun du Sud 53 0.69 30 OR_comwl org Commonwealth 44 0.57 31 OR_ArLig org Ligue arabe 43 0.56 32 CO_AMR_north cont Amérique du Nord 42 0.55 33 CO_AMR_north cont Amérique septentrionale 42 0.55 34 CO_ASI_south_east cont Asie du Sud-Est 42 0.54 35 LA_cauca land Caucase 41 0.54 36 SE_antil sea Antilles 41 0.54 37 LA_himal land Himalaya 40 0.53 38 SE_china_south sea mer de Chine méridionale 40 0.53 39 OR_AfrUn org Union africaine 36 0.47 40 SE_persi sea golfe Persique 34 0.44 41 CO_ASI_centr cont Asie centrale 32 0.43 42 CO_EUR_east cont Europe médiane 32 0.43 43 CO_ASI_pacif cont Asie-Pacifique 28 0.36 44 SE_india sea océan Indien 26 0.34 45 CO_AFR_ssahr cont Afrique subsaharienne 24 0.31 46 CO_AFR_east cont Afrique de l’Est 20 0.26 47 OR_ECWAS org communauté économique des États de l’Afrique de l’Ouest 20 0.26 48 LA_scand land Scandinavie 19 0.25 49 CO_EUR_centr cont Europe centrale 18 0.24 50 CO_EUR_south cont Europe du Sud 18 0.23 8.2.2 Weighted df&lt;-hc_reg[where1 !=&quot;_no_&quot;,list(nb = sum(news_wgt)), list(where1)] df&lt;-merge(tab_def,df,by.x=&quot;code&quot;,by.y=&quot;where1&quot;,all.x=F,all.y=T) df&lt;-df[order(df$nb, decreasing = T),] row.names(df)&lt;-1:dim(df)[1] df$index&lt;-100*df$nb/max(df$nb) kable(head(df,50), digits=c(NA,NA,NA,0,2),row.names = T) code type label nb index 1 OR_EU org Union européenne 7811 100.00 2 CO_EUR cont Europe 5393 69.05 3 CO_AFR cont Afrique 1909 24.45 4 SE_medit sea mer Méditerranée 836 10.71 5 OR_NATO org OTAN 743 9.52 6 LA_east_middle land Moyen-Orient 420 5.37 7 CO_ASI cont Asie 376 4.81 8 SE_black sea mer Noire 358 4.58 9 CO_ASI_minor cont Asie mineure 340 4.35 10 LA_alpen land Alpes 270 3.46 11 LA_sahel land Sahel 262 3.35 12 SE_arcti sea Arctique 210 2.69 13 CO_AMR cont Amérique 203 2.60 14 CO_AMR_latin cont Amérique latine 166 2.13 15 LA_amazon land Amazonie 131 1.68 16 CO_AFR_south cont Afrique australe 127 1.63 17 CO_ANT cont Antarctique 113 1.45 18 LA_east_near land Proche-Orient 102 1.30 19 CO_AMR_south cont Amérique du Sud 98 1.25 20 SE_carai sea Caraïbes 97 1.24 21 LA_maghr land Machrek 92 1.18 22 LA_sahara land Sahara 78 1.00 23 OR_CoEur org Conseil de l’Europe 75 0.96 24 CO_AFR_west cont Afrique de l’Ouest 73 0.93 25 SE_polyn sea Polynésie 73 0.93 26 LA_balka land Balkans 61 0.78 27 OR_ArLig org Ligue arabe 57 0.73 28 OR_Merco org marché commun du Sud 53 0.68 29 CO_ASI_south_east cont Asie du Sud-Est 51 0.65 30 CO_AMR_centr cont Amérique centrale 51 0.65 31 CO_ERA cont Eurasie 50 0.64 32 SE_china_south sea mer de Chine méridionale 48 0.61 33 OR_comwl org Commonwealth 47 0.61 34 LA_himal land Himalaya 46 0.58 35 CO_AMR_north cont Amérique du Nord 39 0.50 36 CO_AMR_north cont Amérique septentrionale 39 0.50 37 OR_AfrUn org Union africaine 37 0.47 38 SE_antil sea Antilles 36 0.46 39 SE_persi sea golfe Persique 35 0.44 40 LA_cauca land Caucase 34 0.44 41 CO_EUR_east cont Europe médiane 33 0.42 42 SE_india sea océan Indien 32 0.41 43 CO_AFR_ssahr cont Afrique subsaharienne 29 0.38 44 CO_ASI_centr cont Asie centrale 26 0.33 45 CO_EUR_centr cont Europe centrale 24 0.31 46 CO_AFR_east cont Afrique de l’Est 24 0.30 47 CO_ASI_pacif cont Asie-Pacifique 23 0.30 48 CO_EUR_south cont Europe du Sud 23 0.29 49 OR_ECWAS org communauté économique des États de l’Afrique de l’Ouest 17 0.22 50 LA_scand land Scandinavie 17 0.21 8.3 Top 10 regions by media 8.3.1 German newspapers - Top 10 regions # Compute df&lt;-hc_reg[where1 !=&quot;_no_&quot;,list(nb = sum(news)), list(who, where1)] %&gt;% group_by(who) %&gt;% filter(where1 != &quot;Q828&quot;)%&gt;% mutate(pct = 100*nb/max(nb), rnk = rank(-nb)) df_sel &lt;- df %&gt;% filter(substr(who,4,6)==&quot;DEU&quot;, rnk &lt; 11) df_sel&lt;-merge(df_sel,tab_def,by.x=&quot;where1&quot;,by.y=&quot;code&quot;) res &lt;- df_sel %&gt;% filter(rnk &lt; 11) %&gt;% select(who, rnk,label, pct) %&gt;% mutate(who=substr(who,4,12)) res&lt;-res[order(res$who, res$rnk),] tab1&lt;-res[1:10,2] names(tab1) &lt;- c(&quot;Rank&quot;) tab2&lt;-res[1:10,c(3,4)] names(tab2)&lt;-c(paste(&quot;FAZ&quot;,&quot;Region&quot;,sep=&quot;_&quot;),paste(&quot;FAZ&quot;,&quot;pct&quot;,sep=&quot; &quot;)) tab3&lt;-res[11:20,c(3,4)] names(tab3)&lt;-c(paste(&quot;Süd. Zeit.&quot;,&quot;Region&quot;,sep=&quot;_&quot;),paste(&quot;Süd. Zeit.&quot;,&quot;pct&quot;,sep=&quot; &quot;)) tab&lt;-cbind(tab1,tab2,tab3) kable(tab,digits=1, row.names = F) tab1 FAZ_Region FAZ pct Süd. Zeit._Region Süd. Zeit. pct 1 Union européenne 100.0 Union européenne 100.0 2 Europe 51.0 Europe 38.6 3 OTAN 10.7 Moyen-Orient 8.1 4 Afrique 7.0 OTAN 7.3 5 Afrique australe 5.1 Afrique 6.7 6 mer Méditerranée 4.1 mer Méditerranée 5.6 7 Asie 3.5 Alpes 3.7 8 Moyen-Orient 3.0 Afrique australe 2.7 9 Alpes 2.9 Proche-Orient 1.9 10 Europe médiane 1.6 Amérique du Sud 1.9 8.3.2 French newspapers - Top 10 regions # Compute df&lt;-hc_reg[where1 !=&quot;_no_&quot;,list(nb = sum(news)), list(who, where1)] %&gt;% group_by(who) %&gt;% filter(where1 != &quot;Q828&quot;)%&gt;% mutate(pct = 100*nb/max(nb), rnk = rank(-nb)) df_sel &lt;- df %&gt;% filter(substr(who,4,6)==&quot;FRA&quot;, rnk &lt; 11) df_sel&lt;-merge(df_sel,tab_def,by.x=&quot;where1&quot;,by.y=&quot;code&quot;) res &lt;- df_sel %&gt;% filter(rnk &lt; 11) %&gt;% select(who, rnk,label, pct) %&gt;% mutate(who=substr(who,4,12)) res&lt;-res[order(res$who, res$rnk),] tab1&lt;-res[1:10,2] names(tab1) &lt;- c(&quot;Rank&quot;) tab2&lt;-res[1:10,c(3,4)] names(tab2)&lt;-c(paste(&quot;Figaro&quot;,&quot;Region&quot;,sep=&quot;_&quot;),paste(&quot;Figaro&quot;,&quot;pct&quot;,sep=&quot; &quot;)) tab3&lt;-res[11:20,c(3,4)] names(tab3)&lt;-c(paste(&quot;Le Monde&quot;,&quot;Region&quot;,sep=&quot;_&quot;),paste(&quot;Le Monde&quot;,&quot;pct&quot;,sep=&quot; &quot;)) tab&lt;-cbind(tab1,tab2,tab3) kable(tab,digits=1, row.names = F) tab1 Figaro_Region Figaro pct Le Monde_Region Le Monde pct 1 Europe 100.0 Europe 100.0 2 Union européenne 98.3 Union européenne 70.5 3 Amérique 15.1 Afrique 41.9 4 mer Méditerranée 14.1 Sahel 13.9 5 Afrique 12.3 mer Méditerranée 11.7 6 Alpes 10.9 Alpes 10.5 7 OTAN 9.4 Amérique 8.9 8 Amazonie 9.4 OTAN 6.9 9 Sahel 8.5 Amazonie 6.0 10 Polynésie 5.9 Proche-Orient 6.0 8.3.3 UK newspapers - Top 10 regions # Compute df&lt;-hc_reg[where1 !=&quot;_no_&quot;,list(nb = sum(news)), list(who, where1)] %&gt;% group_by(who) %&gt;% filter(where1 != &quot;Q828&quot;)%&gt;% mutate(pct = 100*nb/max(nb), rnk = rank(-nb)) df_sel &lt;- df %&gt;% filter(substr(who,4,6)==&quot;GBR&quot;, rnk &lt; 11) df_sel&lt;-merge(df_sel,tab_def,by.x=&quot;where1&quot;,by.y=&quot;code&quot;) res &lt;- df_sel %&gt;% filter(rnk &lt; 11) %&gt;% select(who, rnk,label, pct) %&gt;% mutate(who=substr(who,4,12)) res&lt;-res[order(res$who, res$rnk),] tab1&lt;-res[1:10,2] names(tab1) &lt;- c(&quot;Rank&quot;) tab2&lt;-res[1:10,c(3,4)] names(tab2)&lt;-c(paste(&quot;Guardian&quot;,&quot;Region&quot;,sep=&quot;_&quot;),paste(&quot;Guardian&quot;,&quot;pct&quot;,sep=&quot; &quot;)) tab3&lt;-res[11:20,c(3,4)] names(tab3)&lt;-c(paste(&quot;Daily Telegraph&quot;,&quot;Region&quot;,sep=&quot;_&quot;),paste(&quot;Daily Telegraph&quot;,&quot;pct&quot;,sep=&quot; &quot;)) tab&lt;-cbind(tab1,tab2,tab3) kable(tab,digits=1, row.names = F) tab1 Guardian_Region Guardian pct Daily Telegraph_Region Daily Telegraph pct 1 Union européenne 100.0 Union européenne 100.0 2 Europe 58.4 Europe 52.3 3 Afrique 14.5 Afrique 14.6 4 Arctique 10.1 Asie 5.2 5 Moyen-Orient 9.7 Moyen-Orient 2.9 6 OTAN 6.4 Arctique 2.5 7 Amérique latine 4.6 Commonwealth 2.1 8 Asie 4.4 OTAN 2.1 9 Antarctique 3.8 Caraïbes 2.1 10 Caraïbes 3.3 Amérique latine 1.9 8.3.4 Irish newspapers - Top 10 regions # Compute df&lt;-hc_reg[where1 !=&quot;_no_&quot;,list(nb = sum(news)), list(who, where1)] %&gt;% group_by(who) %&gt;% filter(where1 != &quot;Q828&quot;)%&gt;% mutate(pct = 100*nb/max(nb), rnk = rank(-nb)) df_sel &lt;- df %&gt;% filter(substr(who,4,6) %in% c(&quot;IRL&quot;,&quot;NIR&quot;), rnk &lt; 11) df_sel&lt;-merge(df_sel,tab_def,by.x=&quot;where1&quot;,by.y=&quot;code&quot;) res &lt;- df_sel %&gt;% filter(rnk &lt; 11) %&gt;% select(who, rnk,label, pct) %&gt;% mutate(who=substr(who,4,12)) res&lt;-res[order(res$who, res$rnk),] tab1&lt;-res[1:10,2] names(tab1) &lt;- c(&quot;Rank&quot;) tab2&lt;-res[1:10,c(3,4)] names(tab2)&lt;-c(paste(&quot;Irish Times&quot;,&quot;Region&quot;,sep=&quot;_&quot;),paste(&quot;Irish Times&quot;,&quot;pct&quot;,sep=&quot; &quot;)) tab3&lt;-res[11:20,c(3,4)] names(tab3)&lt;-c(paste(&quot;Belfast Telegraph&quot;,&quot;Region&quot;,sep=&quot;_&quot;),paste(&quot;Belfast Telegraph&quot;,&quot;pct&quot;,sep=&quot; &quot;)) tab&lt;-cbind(tab1,tab2,tab3) kable(tab,digits=1, row.names = F) tab1 Irish Times_Region Irish Times pct Belfast Telegraph_Region Belfast Telegraph pct 1.0 Union européenne 100.0 Union européenne 100.0 2.0 Europe 31.0 Europe 30.7 3.0 Afrique 2.9 Afrique 3.7 4.0 OTAN 2.3 Commonwealth 3.1 5.0 Asie 2.1 Arctique 3.1 6.0 Moyen-Orient 1.8 OTAN 2.5 7.0 mer Méditerranée 1.4 Moyen-Orient 2.4 8.0 Machrek 1.0 Asie 2.3 9.5 Amérique latine 0.8 Caraïbes 1.7 9.5 Alpes 0.8 mer Méditerranée 0.9 8.3.5 Turkish newspapers - Top 10 regions # Compute df&lt;-hc_reg[where1 !=&quot;_no_&quot;,list(nb = sum(news)), list(who, where1)] %&gt;% group_by(who) %&gt;% # filter(where1 != &quot;Q828&quot;)%&gt;% mutate(pct = 100*nb/max(nb), rnk = rank(-nb)) df_sel &lt;- df %&gt;% filter(substr(who,4,6)==&quot;TUR&quot;, rnk &lt; 11) df_sel&lt;-merge(df_sel,tab_def,by.x=&quot;where1&quot;,by.y=&quot;code&quot;) res &lt;- df_sel %&gt;% filter(rnk &lt; 11) %&gt;% select(who, rnk,label, pct) %&gt;% mutate(who=substr(who,4,12)) res&lt;-res[order(res$who, res$rnk),] tab1&lt;-res[1:10,2] names(tab1) &lt;- c(&quot;Rank&quot;) tab2&lt;-res[1:10,c(3,4)] names(tab2)&lt;-c(paste(&quot;Cumhuryet&quot;,&quot;Region&quot;,sep=&quot;_&quot;),paste(&quot;Cumhuryet&quot;,&quot;pct&quot;,sep=&quot; &quot;)) tab3&lt;-res[11:20,c(3,4)] names(tab3)&lt;-c(paste(&quot;Yeni Savak&quot;,&quot;Region&quot;,sep=&quot;_&quot;),paste(&quot;Yeni Savak&quot;,&quot;pct&quot;,sep=&quot; &quot;)) tab&lt;-cbind(tab1,tab2,tab3) kable(tab,digits=1, row.names = F) tab1 Cumhuryet_Region Cumhuryet pct Yeni Savak_Region Yeni Savak pct 1 Europe 100.0 Europe 100.0 2 Union européenne 68.5 Union européenne 82.8 3 Asie mineure 39.3 mer Méditerranée 53.3 4 mer Méditerranée 31.4 mer Noire 41.0 5 OTAN 24.8 OTAN 34.1 6 mer Noire 23.0 Asie mineure 28.4 7 Afrique 7.9 Afrique 17.5 8 Asie 5.9 Asie 6.3 9 Eurasie 4.4 Eurasie 5.3 10 Afrique australe 3.8 Antarctique 3.2 8.3.6 Algerian newspapers # Compute df&lt;-hc_reg[where1 !=&quot;_no_&quot;,list(nb = sum(news)), list(who, where1)] %&gt;% group_by(who) %&gt;% filter(where1 != &quot;Q828&quot;)%&gt;% mutate(pct = 100*nb/max(nb), rnk = rank(-nb)) df_sel &lt;- df %&gt;% filter(substr(who,4,6)==&quot;DZA&quot;, rnk &lt; 11) df_sel&lt;-merge(df_sel,tab_def,by.x=&quot;where1&quot;,by.y=&quot;code&quot;) res &lt;- df_sel %&gt;% filter(rnk &lt; 11) %&gt;% select(who, rnk,label, pct) %&gt;% mutate(who=substr(who,4,12)) res&lt;-res[order(res$who, res$rnk),] tab1&lt;-res[1:10,2] names(tab1) &lt;- c(&quot;Rank&quot;) tab2&lt;-res[1:10,c(3,4)] names(tab2)&lt;-c(&quot;Al Nahar (ar)&quot;,&quot;pct1&quot;) tab3&lt;-res[12:21,c(3,4)] names(tab3)&lt;-c(&quot;El Kahbar (ar)&quot;,&quot;pct2&quot;) tab&lt;-cbind(tab1,tab2,tab3) kable(tab,digits=c(0,1,1,1,1,1), row.names = F) tab1 Al Nahar (ar) pct1 El Kahbar (ar) pct2 1 Afrique 100.0 Afrique 100.0 2 Europe 68.7 Europe 34.9 3 Union européenne 29.2 Union européenne 13.7 4 Sahel 10.6 mer Méditerranée 6.4 5 Asie 9.6 Asie 5.7 6 Moyen-Orient 6.3 Moyen-Orient 4.6 6 Ligue arabe 6.3 Machrek 3.2 8 mer Méditerranée 5.1 Sahel 3.2 10 Amazonie 2.9 Ligue arabe 2.1 10 Machrek 2.9 OTAN 1.4 8.4 Synthesis by correspondance analysis and Hierarchical Clustering The synthesis is realized with regions 8.4.1 Factor 1-2 # Matrix reg_med &lt;-hc_reg[where1 !=&quot;_no_&quot;,list(nb = sum(news_wgt)), list(where1, who)] %&gt;% dcast(formula = where1~who, value.var = &quot;nb&quot;,fill = 0) %&gt;% filter(where1 !=&quot;CO_AFR_south&quot;) mat&lt;-as.matrix(reg_med[,-1]) # Labels lab&lt;-reg_med[,1] %&gt;% rename(code=where1) %&gt;% unique() lab&lt;-merge(lab,tab_def,all.x=T,all.y=F) %&gt;% filter(duplicated(code)==F) # Row.names (choose the language you want !) row.names(mat)&lt;-lab$label # Filter ambiguous units #mat&lt;-mat[row.names(mat) != &quot;Americas&quot;,] #mat&lt;-mat[row.names(mat) != &quot;Asia Minor&quot;,] #mat&lt;-mat[row.names(mat) != &quot;Southern Africa&quot;,] #mat&lt;-mat[row.names(mat) != &quot;Europe&quot;,] #mat&lt;-mat[row.names(mat) != &quot;European Union&quot;,] # Select units &gt; 40 sel&lt;-mat[apply(mat,1,sum)&gt;20,] # Exclude units mentionned by less than 3 media sel &lt;- sel[apply(sel&gt;3,1,sum)&gt;2,] afc &lt;- CA(sel, graph = F) #library(explor) #explor(afc) res &lt;- explor::prepare_results(afc) explor::CA_var_plot(res, xax = 1, yax = 2, lev_sup = FALSE, var_sup = FALSE, var_sup_choice = , var_hide = &quot;None&quot;, var_lab_min_contrib = 0, col_var = &quot;Position&quot;, symbol_var = NULL, size_var = &quot;Contrib&quot;, size_range = c(52.5, 700), labels_size = 8, point_size = 56, transitions = TRUE, labels_positions = &quot;auto&quot;, xlim = c(-2, 1), ylim = c(-1, 2.1)) 8.4.2 Factors 3-4 res &lt;- explor::prepare_results(afc) explor::CA_var_plot(res, xax = 3, yax = 4, lev_sup = FALSE, var_sup = FALSE, var_sup_choice = , var_hide = &quot;None&quot;, var_lab_min_contrib = 0, col_var = &quot;Position&quot;, symbol_var = NULL, size_var = &quot;Contrib&quot;, size_range = c(52.5, 700), labels_size = 8, point_size = 56, transitions = TRUE, labels_positions = &quot;auto&quot;, xlim = c(-2.17, 1.7), ylim = c(-1.86, 2.01)) 8.4.3 Cluster analysis (world regions) cah1 &lt;- HCPC(afc,nb.clust = 5,graph = FALSE) plot.HCPC(cah1,choice=&quot;tree&quot;) 8.4.4 Cluster analysis (medias) cah2 &lt;- HCPC(afc,nb.clust = 4,graph = FALSE,cluster.CA = &quot;columns&quot;) plot.HCPC(cah2,choice=&quot;tree&quot;) 8.5 Synthesis by chi-square &amp; heatmap An alternative approach based on the computation of chi-square and the classification of residuals (censoredd between -3 and +3) library(pheatmap) x&lt;-chisq.test(sel) Warning in chisq.test(sel): Chi-squared approximation may be incorrect res&lt;-as.matrix(x$residuals) exp&lt;-as.matrix(x$expected) res[exp&lt;2]&lt;-0 res[res&gt;3]&lt;-3 res[res&lt; -3]&lt;- -3 #pdf(file = &quot;test.pdf&quot;) pheatmap(res, cutree_rows = 7,cutree_cols = 6) #dev.off() "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
=======
[["23-Taggeo.html", "Chapter 6 Geographical tags 6.1 Manual correctio n of dictionnary 6.2 Detection of geographical entities", " Chapter 6 Geographical tags library(knitr) library(dplyr) library(WikidataR) Warning: le package &#39;WikidataR&#39; a été compilé avec la version R 4.2.1 library(quanteda) 6.1 Manual correctio n of dictionnary The dictionnary elaboated by automatic procedures has been manually corrected and the new version is upload. dict&lt;-read.table(&quot;dict/worldgeo_dict_V4bis.csv&quot;, header=T, sep=&quot;;&quot;, encoding = &quot;UTF-8&quot;, quote = &#39;&quot;&#39;) 6.2 Detection of geographical entities We have elaborated a function for the extraction of geographical units based on the dictionary elaborated in previous section (dict) according to the language (lang), the decision to split some tokens (split) to move or not to lower case (tolow) and the possibility to add a list of compounds to be realized (comps) in order to eliminate ambiguities. extract_tags &lt;- function(qd = qd, # the corpus of interest lang = &quot;fr&quot;, # the language to be used dict = dict, # the dictionary of target code = &quot;id&quot; , # variable used for coding split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), # split list tolow = FALSE , # Tokenize text comps = c(&quot;Afrique du sud&quot;) # compounds ) { # Tokenize x&lt;-as.character(qd) if(length(split) &gt; 0) { reg&lt;-paste(split, collapse = &#39;|&#39;) x &lt;- gsub(reg,&quot; &quot;,x)} if(tolow) { x &lt;- tolower(x)} toks&lt;-tokens(x) # compounds if(length(split) &gt; 0) { reg&lt;-paste(split, collapse = &#39;|&#39;) comps&lt;- gsub(reg,&quot; &quot;,comps)} if(tolow) {comps &lt;- tolower(comps)} toks&lt;-tokens_compound(toks,pattern=phrase(comps)) # Load dictionaries and create compounds ## Target dictionary dict&lt;-dict[dict$lang==lang &amp; is.na(dict$label)==F,] target&lt;-dict[ntoken(dict$label)&gt;1,] labels &lt;-dict$label if(length(split) &gt; 0) { reg&lt;-paste(split, collapse = &#39;|&#39;) labels&lt;- gsub(reg,&quot; &quot;,labels)} if(tolow) {labels &lt;- tolower(labels)} toks&lt;-tokens_compound(toks,pattern=phrase(labels)) # create quanteda dictionary keys &lt;-gsub(&quot; &quot;,&quot;_&quot;,labels) qd_dict&lt;-as.list(keys) names(qd_dict)&lt;-dict[[code]] qd_dict&lt;-dictionary(qd_dict,tolower = FALSE) # Identify geo tags (states or reg or org ...) toks_tags &lt;- tokens_lookup(toks, qd_dict, case_insensitive = F) toks_tags &lt;- lapply(toks_tags, unique) toks_tags&lt;-as.tokens(toks_tags) list_tags&lt;-function(x){res&lt;-paste(x, collapse=&#39; &#39;)} docvars(qd)[[&quot;tags&quot;]]&lt;-as.character(lapply(toks_tags,FUN=list_tags)) docvars(qd)[[&quot;nbtags&quot;]]&lt;-ntoken(toks_tags) # Export results return(qd) } 6.2.1 Le Figaro (FRA) #dict&lt;-readRDS(&quot;dict/worldgeo_dict_V4.RDS&quot;) qd &lt;- readRDS(&quot;quanteda/fr_FRA_figaro.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) frcomps&lt;-c(&quot;Europe 1&quot;, &quot;Atlantic city&quot;, &quot;Loire-Atlantique&quot;, &quot;Pyrénées-Atlantique&quot;, &quot;Pyrénées-Atlantiques&quot;, &quot;Alpes-de-Haute-Provence&quot;, &quot;Hautes-Alpes&quot;, &quot;Rhöne-Alpes&quot;,&quot;Alpes-Maritimes&quot;, &quot;Chantiers de l&#39;Atlantique&quot;, &quot;TGV Atlantique&quot;, &quot;Bourse de Paris&quot;, &quot;Paris SG&quot;, &quot;Ville de Paris&quot;, &quot;Grand Paris&quot;) qd &lt;- extract_tags (qd = qd, lang=&quot;fr&quot;, dict = dict, code = &quot;code&quot;, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), comps = frcomps, tolow = FALSE) saveRDS(qd,&quot;quanteda/fr_FRA_figaro_geo.RDS&quot;) qd1&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),regs=qd$tags,nbregs=qd$nbtags) 6.2.2 Le Monde qd &lt;- readRDS(&quot;quanteda/fr_FRA_lmonde.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;fr&quot;, dict = dict, code = &quot;code&quot;, comps = frcomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/fr_FRA_lmonde_geo.RDS&quot;) qd2&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.3 FAZ qd &lt;- readRDS(&quot;quanteda/de_DEU_frankf.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) decomps &lt;- c(&quot;Europa League&quot;) qd &lt;- extract_tags (qd = qd, lang=&quot;de&quot;, dict = dict, code = &quot;code&quot;, comps = decomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/de_DEU_frankf_geo.RDS&quot;) qd3&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.4 Süddeutsche Zeitung qd &lt;- readRDS(&quot;quanteda/de_DEU_suddeu.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;de&quot;, dict = dict, code = &quot;code&quot;, comps = decomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/de_DEU_suddeu_geo.RDS&quot;) qd4&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.5 Guardian qd &lt;- readRDS(&quot;quanteda/en_GBR_guardi.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) encomps&lt;-c(&quot;Atlantic City&quot;, &quot;Cathay Pacific&quot;, &quot;Virgin Atlantic&quot;) qd &lt;- extract_tags (qd = qd, lang=&quot;en&quot;, dict = dict, code = &quot;code&quot;, comps = encomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/en_GBR_guardi_geo.RDS&quot;) qd5&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.6 Daily Telegraph qd &lt;- readRDS(&quot;quanteda/en_GBR_telegr.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;en&quot;, dict = dict, code = &quot;code&quot;, comps = encomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/en_GBR_telegr_geo.RDS&quot;) qd6&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.7 Belfast Telegraph qd &lt;- readRDS(&quot;quanteda/en_NIR_beltel.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;en&quot;, dict = dict, code = &quot;code&quot;, comps = encomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/en_NIR_beltel_geo.RDS&quot;) qd7&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.8 Irish Times qd &lt;- readRDS(&quot;quanteda/en_IRL_irtime.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;en&quot;, dict = dict, code = &quot;code&quot;, comps = encomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/en_IRL_irtime_geo.RDS&quot;) qd8&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.9 Cumhuryet qd &lt;- readRDS(&quot;quanteda/tr_TUR_cumhur.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) trcomps &lt;- c(&quot;Cathay Pacific&quot;, &quot;Avrupa Ligi&quot;) qd &lt;- extract_tags (qd = qd, lang=&quot;tr&quot;, dict = dict, code = &quot;code&quot;, comps = trcomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/tr_TUR_cumhur_geo.RDS&quot;) qd9&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.10 Yeni Safak qd &lt;- readRDS(&quot;quanteda/tr_TUR_yenisa.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;tr&quot;, dict = dict, code = &quot;code&quot;, comps = trcomps, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/tr_TUR_yenisa_geo.RDS&quot;) qd10&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.11 El Khabar (DZA) qd &lt;- readRDS(&quot;quanteda/ar_DZA_elkahb.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;ar&quot;, dict = dict, code = &quot;code&quot;, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/ar_DZA_elkahb_geo.RDS&quot;) qd11&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.12 Al Chorouk (DZA) qd &lt;- readRDS(&quot;quanteda/ar_DZA_chorou.RDS&quot;) docvars(qd)&lt;-docvars(qd)[c(&quot;source&quot;,&quot;date&quot;)] qd&lt;-corpus_subset(qd,duplicated(as.character(qd))==FALSE) qd &lt;- extract_tags (qd = qd, lang=&quot;ar&quot;, dict = dict, code = &quot;code&quot;, split = c(&quot;&#39;&quot;,&quot;’&quot;,&quot;-&quot;), tolow = FALSE) saveRDS(qd,&quot;quanteda/ar_DZA_chorou_geo.RDS&quot;) qd12&lt;-qd table(qd$nbtags) x&lt;-data.frame(text=as.character(qd),tags=qd$tags,nbtags=qd$nbtags) 6.2.13 Global corpus qd &lt;- c(qd1,qd2,qd3,qd4,qd5,qd6,qd7,qd8,qd9,qd10,qd11,qd12) saveRDS(qd,&quot;quanteda/corpus_worldgeo_V3.RDS&quot;) table(qd$nbtags) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
>>>>>>> 9b921198f393f3421c5fc618956bd746991d0d8b
